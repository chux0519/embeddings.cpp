{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7af4a7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgguf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GGUFWriter, GGMLQuantizationType\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n\u001b[32m      8\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mhttp_proxy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mhttp://127.0.0.1:2080\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mhttps_proxy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mhttp://127.0.0.1:2080\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuxd\\repos\\embeddings.cpp\\.venv\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     logging,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m define_import_structure\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuxd\\repos\\embeddings.cpp\\.venv\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuxd\\repos\\embeddings.cpp\\.venv\\Lib\\site-packages\\transformers\\utils\\__init__.py:19\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lru_cache\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_full_repo_name  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HF_HUB_DISABLE_TELEMETRY \u001b[38;5;28;01mas\u001b[39;00m DISABLE_TELEMETRY  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuxd\\repos\\embeddings.cpp\\.venv\\Lib\\site-packages\\huggingface_hub\\__init__.py:998\u001b[39m, in \u001b[36m_attach.<locals>.__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    996\u001b[39m submod_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m998\u001b[39m     submod = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError importing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmod_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.4-windows-x86_64-none\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuxd\\repos\\embeddings.cpp\\.venv\\Lib\\site-packages\\huggingface_hub\\hf_api.py:49\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     TYPE_CHECKING,\n\u001b[32m     32\u001b[39m     Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     overload,\n\u001b[32m     46\u001b[39m )\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quote, unquote\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m base_tqdm\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuxd\\repos\\embeddings.cpp\\.venv\\Lib\\site-packages\\requests\\__init__.py:164\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m packages, utils\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__version__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    153\u001b[39m     __author__,\n\u001b[32m    154\u001b[39m     __author_email__,\n\u001b[32m   (...)\u001b[39m\u001b[32m    162\u001b[39m     __version__,\n\u001b[32m    163\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m delete, get, head, options, patch, post, put, request\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    166\u001b[39m     \u001b[38;5;167;01mConnectionError\u001b[39;00m,\n\u001b[32m    167\u001b[39m     ConnectTimeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m     URLRequired,\n\u001b[32m    176\u001b[39m )\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreparedRequest, Request, Response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuxd\\repos\\embeddings.cpp\\.venv\\Lib\\site-packages\\requests\\api.py:11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mrequests.api\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m~~~~~~~~~~~~\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03m:license: Apache2, see LICENSE for more details.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sessions\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(method, url, **kwargs):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Constructs and sends a :class:`Request <Request>`.\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03m    :param method: method for the new :class:`Request` object: ``GET``, ``OPTIONS``, ``HEAD``, ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m \u001b[33;03m      <Response [200]>\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuxd\\repos\\embeddings.cpp\\.venv\\Lib\\site-packages\\requests\\sessions.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m timedelta\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_internal_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_native_string\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01madapters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTTPAdapter\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _basic_auth_str\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Mapping, cookielib, urljoin, urlparse\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chuxd\\repos\\embeddings.cpp\\.venv\\Lib\\site-packages\\requests\\adapters.py:81\u001b[39m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mssl\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     80\u001b[39m     _preloaded_ssl_context = create_urllib3_context()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43m_preloaded_ssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextract_zipped_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEFAULT_CA_BUNDLE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# Bypass default SSLContext creation when Python\u001b[39;00m\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# interpreter isn't built with the ssl module.\u001b[39;00m\n\u001b[32m     87\u001b[39m     _preloaded_ssl_context = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from gguf import GGUFWriter, GGMLQuantizationType\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:2080\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:2080\"\n",
    "\n",
    "def convert_hf(repo_id, output_path, float_type='f16'):\n",
    "    # convert to ggml quantization type\n",
    "    if float_type not in ['f16', 'f32']:\n",
    "        print(f'Float type must be f16 or f32, got: {float_type}')\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        qtype = GGMLQuantizationType[float_type.upper()]\n",
    "        dtype0 = {'f16': torch.float16, 'f32': torch.float32}[float_type]\n",
    "\n",
    "    # load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "    tokenizer_json = os.path.join(os.path.dirname(output_path), os.path.basename(repo_id) + \".tokenizer.json\")\n",
    "    # tokenizer.save_pretrained(tokenizer_json)\n",
    "    tokenizer._tokenizer.save(tokenizer_json, False)\n",
    "    model = AutoModel.from_pretrained(repo_id, add_pooling_layer=False, trust_remote_code=True)\n",
    "\n",
    "    config = model.config\n",
    "    print(config)\n",
    "    \n",
    "    # print model\n",
    "    param_keys = [\n",
    "        'vocab_size', 'hidden_size', 'num_hidden_layers',\n",
    "        'num_attention_heads', 'intermediate_size', 'type_vocab_size', 'pad_token_id'\n",
    "    ]\n",
    "    print('PARAMS')\n",
    "    for k in param_keys:\n",
    "        v = getattr(config, k)\n",
    "        print(f'{k:<24s} = {v}')\n",
    "    print()\n",
    "\n",
    "    # print vocab\n",
    "    vocab_keys = [\n",
    "        'vocab_size', 'pad_token_id', 'unk_token_id', 'cls_token_id', 'sep_token_id'\n",
    "    ]\n",
    "    print('VOCAB')\n",
    "    for k in vocab_keys:\n",
    "        v = getattr(tokenizer, k)\n",
    "        print(f'{k:24s} = {v}')\n",
    "\n",
    "\n",
    "    # start to write GGUF file\n",
    "    gguf_writer = GGUFWriter(output_path, \"JinaBert\")\n",
    "\n",
    "    # write metadata\n",
    "    gguf_writer.add_name(repo_id)\n",
    "    gguf_writer.add_description('gguf model for embeddings.cpp')\n",
    "    gguf_writer.add_file_type(qtype)\n",
    "\n",
    "    # write model params\n",
    "    gguf_writer.add_uint32('vocab_size', config.vocab_size)\n",
    "    gguf_writer.add_uint32('hidden_size', config.hidden_size)\n",
    "    gguf_writer.add_uint32('intermediate_size', config.intermediate_size)\n",
    "    gguf_writer.add_uint32('num_attention_heads', config.num_attention_heads)\n",
    "    gguf_writer.add_uint32('num_hidden_layers', config.num_hidden_layers)\n",
    "    gguf_writer.add_uint32('type_vocab_size', config.type_vocab_size)\n",
    "    gguf_writer.add_uint32('pad_token_id', config.pad_token_id)\n",
    "    gguf_writer.add_float32('layer_norm_eps', config.layer_norm_eps)\n",
    "    gguf_writer.add_float32('rope_theta', config.rope_theta)\n",
    "\n",
    "\n",
    "    # write the tokenizer special token(we only need to know [PAD])\n",
    "    KEY_PAD_ID = 'tokenizer.ggml.padding_token_id'\n",
    "    gguf_writer.add_int32(KEY_PAD_ID, tokenizer.pad_token_id)\n",
    "\n",
    "    # write tensors\n",
    "    print('TENSORS')\n",
    "    hidden_size = config.hidden_size\n",
    "    for name, data in model.state_dict().items():\n",
    "        # get correct dtype\n",
    "        if 'attn_ln' in name or 'mlp' in name  or 'bias' in name or 'proj' in name or 'LayerNorm' in name:\n",
    "            dtype = torch.float32\n",
    "        else:\n",
    "            dtype = dtype0\n",
    "        shape_str = str(list(data.shape))\n",
    "        print(f'{name:64s} = {shape_str:16s} {data.dtype} → {dtype}')\n",
    "\n",
    "        # do conversion\n",
    "        data = data.to(dtype)\n",
    "\n",
    "        # add to gguf output\n",
    "        gguf_writer.add_tensor(name, data.numpy())\n",
    "\n",
    "    # execute and close writer\n",
    "    gguf_writer.write_header_to_file()\n",
    "    gguf_writer.write_kv_data_to_file()\n",
    "    gguf_writer.write_tensors_to_file()\n",
    "    gguf_writer.close()\n",
    "\n",
    "    # print success\n",
    "    print()\n",
    "    print(f'GGML model written to {output_path}')\n",
    "\n",
    "repo_id = 'Snowflake/snowflake-arctic-embed-m-v2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e875382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GteConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"GteModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"Snowflake/snowflake-arctic-embed-m-v2.0--configuration_hf_alibaba_nlp_gte.GteConfig\",\n",
      "    \"AutoModel\": \"Snowflake/snowflake-arctic-embed-m-v2.0--modeling_hf_alibaba_nlp_gte.GteModel\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_norm_type\": \"layer_norm\",\n",
      "  \"logn_attention_clip1\": false,\n",
      "  \"logn_attention_scale\": false,\n",
      "  \"matryoshka_dimensions\": [\n",
      "    256\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gte\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pack_qkv\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rope\",\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 160000,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"unpad_inputs\": \"true\",\n",
      "  \"use_memory_efficient_attention\": \"true\",\n",
      "  \"vocab_size\": 250048\n",
      "}\n",
      "\n",
      "PARAMS\n",
      "vocab_size               = 250048\n",
      "hidden_size              = 768\n",
      "num_hidden_layers        = 12\n",
      "num_attention_heads      = 12\n",
      "intermediate_size        = 3072\n",
      "type_vocab_size          = 1\n",
      "pad_token_id             = 1\n",
      "\n",
      "VOCAB\n",
      "vocab_size               = 250002\n",
      "pad_token_id             = 1\n",
      "unk_token_id             = 3\n",
      "cls_token_id             = 0\n",
      "sep_token_id             = 2\n",
      "TENSORS\n",
      "embeddings.word_embeddings.weight                                = [250048, 768]    torch.float32 → torch.float16\n",
      "embeddings.token_type_embeddings.weight                          = [1, 768]         torch.float32 → torch.float16\n",
      "embeddings.LayerNorm.weight                                      = [768]            torch.float32 → torch.float32\n",
      "embeddings.LayerNorm.bias                                        = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.0.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.0.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.0.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.0.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.0.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.1.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.1.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.1.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.1.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.1.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.2.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.2.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.2.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.2.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.2.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.3.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.3.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.3.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.3.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.3.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.4.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.4.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.4.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.4.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.4.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.5.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.5.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.5.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.5.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.5.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.6.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.6.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.6.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.6.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.6.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.7.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.7.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.7.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.7.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.7.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.8.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.8.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.8.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.8.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.8.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.9.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.9.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.9.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.9.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.9.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.attention.qkv_proj.weight                       = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.10.attention.qkv_proj.bias                         = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.10.attention.o_proj.weight                         = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.10.attention.o_proj.bias                           = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.mlp.up_gate_proj.weight                         = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.10.mlp.down_proj.weight                            = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.10.mlp.down_proj.bias                              = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.attn_ln.weight                                  = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.attn_ln.bias                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.mlp_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.mlp_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.attention.qkv_proj.weight                       = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.11.attention.qkv_proj.bias                         = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.11.attention.o_proj.weight                         = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.11.attention.o_proj.bias                           = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.mlp.up_gate_proj.weight                         = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.11.mlp.down_proj.weight                            = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.11.mlp.down_proj.bias                              = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.attn_ln.weight                                  = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.attn_ln.bias                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.mlp_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.mlp_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "\n",
      "GGML model written to ../models/snowflake-arctic-embed-m-v2.0.fp16.gguf\n"
     ]
    }
   ],
   "source": [
    "convert_hf(repo_id, \"../models/snowflake-arctic-embed-m-v2.0.fp16.gguf\", float_type=\"f16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abaa8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chuxd\\repos\\embeddings.cpp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,    62, 57571,  7515,     5,     2],\n",
      "        [    0,    62, 57571,  7515,     2,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0]])}\n",
      "unpad_inputs: true, attention_mask: torch.Size([2, 6]), length: None\n",
      "input_ids: tensor([[    0,    62, 57571,  7515,     5,     2],\n",
      "        [    0,    62, 57571,  7515,     2,     1]])\n",
      "attention_mask_bool: tensor([[ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True, False]])\n",
      "input_ids after unpadding: tensor([[    0,    62, 57571,  7515,     5,     2,     0,    62, 57571,  7515,\n",
      "             2]])\n",
      "Using unpadded position_ids\n",
      "position_ids: None\n",
      "position_ids after unpadding: tensor([[0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4]])\n",
      "position_ids: tensor([[0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4]])\n",
      "seq_length: 6\n",
      "rope_cos: tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.7727,  0.8903,  0.9476,  0.9751,  0.9882,  0.9944,  0.9974,\n",
      "          0.9988,  0.9994,  0.9997,  0.9999,  0.9999,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          0.5403,  0.7727,  0.8903,  0.9476,  0.9751,  0.9882,  0.9944,  0.9974,\n",
      "          0.9988,  0.9994,  0.9997,  0.9999,  0.9999,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.4161,  0.1942,  0.5851,  0.7959,  0.9017,  0.9531,  0.9777,  0.9894,\n",
      "          0.9950,  0.9976,  0.9989,  0.9995,  0.9998,  0.9999,  0.9999,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -0.4161,  0.1942,  0.5851,  0.7959,  0.9017,  0.9531,  0.9777,  0.9894,\n",
      "          0.9950,  0.9976,  0.9989,  0.9995,  0.9998,  0.9999,  0.9999,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.9900, -0.4725,  0.1516,  0.5607,  0.7833,  0.8955,  0.9501,  0.9763,\n",
      "          0.9888,  0.9947,  0.9975,  0.9988,  0.9994,  0.9997,  0.9999,  0.9999,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -0.9900, -0.4725,  0.1516,  0.5607,  0.7833,  0.8955,  0.9501,  0.9763,\n",
      "          0.9888,  0.9947,  0.9975,  0.9988,  0.9994,  0.9997,  0.9999,  0.9999,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [-0.6536, -0.9245, -0.3152,  0.2668,  0.6260,  0.8167,  0.9119,  0.9580,\n",
      "          0.9801,  0.9906,  0.9955,  0.9979,  0.9990,  0.9995,  0.9998,  0.9999,\n",
      "          0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         -0.6536, -0.9245, -0.3152,  0.2668,  0.6260,  0.8167,  0.9119,  0.9580,\n",
      "          0.9801,  0.9906,  0.9955,  0.9979,  0.9990,  0.9995,  0.9998,  0.9999,\n",
      "          0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
      "        [ 0.2837, -0.9563, -0.7129, -0.0550,  0.4375,  0.7187,  0.8635,  0.9346,\n",
      "          0.9689,  0.9853,  0.9930,  0.9967,  0.9984,  0.9993,  0.9997,  0.9998,\n",
      "          0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          0.2837, -0.9563, -0.7129, -0.0550,  0.4375,  0.7187,  0.8635,  0.9346,\n",
      "          0.9689,  0.9853,  0.9930,  0.9967,  0.9984,  0.9993,  0.9997,  0.9998,\n",
      "          0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "          1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]]), rope_sin: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 8.4147e-01,  6.3473e-01,  4.5544e-01,  3.1947e-01,  2.2175e-01,\n",
      "          1.5316e-01,  1.0554e-01,  7.2647e-02,  4.9979e-02,  3.4376e-02,\n",
      "          2.3641e-02,  1.6258e-02,  1.1180e-02,  7.6882e-03,  5.2868e-03,\n",
      "          3.6355e-03,  2.5000e-03,  1.7191e-03,  1.1822e-03,  8.1293e-04,\n",
      "          5.5902e-04,  3.8441e-04,  2.6434e-04,  1.8178e-04,  1.2500e-04,\n",
      "          8.5957e-05,  5.9109e-05,  4.0647e-05,  2.7951e-05,  1.9221e-05,\n",
      "          1.3217e-05,  9.0888e-06,  8.4147e-01,  6.3473e-01,  4.5544e-01,\n",
      "          3.1947e-01,  2.2175e-01,  1.5316e-01,  1.0554e-01,  7.2647e-02,\n",
      "          4.9979e-02,  3.4376e-02,  2.3641e-02,  1.6258e-02,  1.1180e-02,\n",
      "          7.6882e-03,  5.2868e-03,  3.6355e-03,  2.5000e-03,  1.7191e-03,\n",
      "          1.1822e-03,  8.1293e-04,  5.5902e-04,  3.8441e-04,  2.6434e-04,\n",
      "          1.8178e-04,  1.2500e-04,  8.5957e-05,  5.9109e-05,  4.0647e-05,\n",
      "          2.7951e-05,  1.9221e-05,  1.3217e-05,  9.0888e-06],\n",
      "        [ 9.0930e-01,  9.8095e-01,  8.1093e-01,  6.0546e-01,  4.3245e-01,\n",
      "          3.0270e-01,  2.0990e-01,  1.4491e-01,  9.9833e-02,  6.8711e-02,\n",
      "          4.7269e-02,  3.2512e-02,  2.2359e-02,  1.5376e-02,  1.0574e-02,\n",
      "          7.2710e-03,  5.0000e-03,  3.4383e-03,  2.3644e-03,  1.6259e-03,\n",
      "          1.1180e-03,  7.6882e-04,  5.2869e-04,  3.6355e-04,  2.5000e-04,\n",
      "          1.7191e-04,  1.1822e-04,  8.1293e-05,  5.5902e-05,  3.8441e-05,\n",
      "          2.6434e-05,  1.8178e-05,  9.0930e-01,  9.8095e-01,  8.1093e-01,\n",
      "          6.0546e-01,  4.3245e-01,  3.0270e-01,  2.0990e-01,  1.4491e-01,\n",
      "          9.9833e-02,  6.8711e-02,  4.7269e-02,  3.2512e-02,  2.2359e-02,\n",
      "          1.5376e-02,  1.0574e-02,  7.2710e-03,  5.0000e-03,  3.4383e-03,\n",
      "          2.3644e-03,  1.6259e-03,  1.1180e-03,  7.6882e-04,  5.2869e-04,\n",
      "          3.6355e-04,  2.5000e-04,  1.7191e-04,  1.1822e-04,  8.1293e-05,\n",
      "          5.5902e-05,  3.8441e-05,  2.6434e-05,  1.8178e-05],\n",
      "        [ 1.4112e-01,  8.8131e-01,  9.8844e-01,  8.2799e-01,  6.2163e-01,\n",
      "          4.4511e-01,  3.1192e-01,  2.1641e-01,  1.4944e-01,  1.0297e-01,\n",
      "          7.0871e-02,  4.8757e-02,  3.3535e-02,  2.3063e-02,  1.5860e-02,\n",
      "          1.0906e-02,  7.4999e-03,  5.1574e-03,  3.5465e-03,  2.4388e-03,\n",
      "          1.6771e-03,  1.1532e-03,  7.9303e-04,  5.4533e-04,  3.7500e-04,\n",
      "          2.5787e-04,  1.7733e-04,  1.2194e-04,  8.3853e-05,  5.7662e-05,\n",
      "          3.9651e-05,  2.7267e-05,  1.4112e-01,  8.8131e-01,  9.8844e-01,\n",
      "          8.2799e-01,  6.2163e-01,  4.4511e-01,  3.1192e-01,  2.1641e-01,\n",
      "          1.4944e-01,  1.0297e-01,  7.0871e-02,  4.8757e-02,  3.3535e-02,\n",
      "          2.3063e-02,  1.5860e-02,  1.0906e-02,  7.4999e-03,  5.1574e-03,\n",
      "          3.5465e-03,  2.4388e-03,  1.6771e-03,  1.1532e-03,  7.9303e-04,\n",
      "          5.4533e-04,  3.7500e-04,  2.5787e-04,  1.7733e-04,  1.2194e-04,\n",
      "          8.3853e-05,  5.7662e-05,  3.9651e-05,  2.7267e-05],\n",
      "        [-7.5680e-01,  3.8108e-01,  9.4902e-01,  9.6374e-01,  7.7985e-01,\n",
      "          5.7701e-01,  4.1045e-01,  2.8676e-01,  1.9867e-01,  1.3710e-01,\n",
      "          9.4433e-02,  6.4989e-02,  4.4706e-02,  3.0748e-02,  2.1146e-02,\n",
      "          1.4542e-02,  9.9998e-03,  6.8765e-03,  4.7287e-03,  3.2517e-03,\n",
      "          2.2361e-03,  1.5376e-03,  1.0574e-03,  7.2711e-04,  5.0000e-04,\n",
      "          3.4383e-04,  2.3644e-04,  1.6259e-04,  1.1180e-04,  7.6882e-05,\n",
      "          5.2869e-05,  3.6355e-05, -7.5680e-01,  3.8108e-01,  9.4902e-01,\n",
      "          9.6374e-01,  7.7985e-01,  5.7701e-01,  4.1045e-01,  2.8676e-01,\n",
      "          1.9867e-01,  1.3710e-01,  9.4433e-02,  6.4989e-02,  4.4706e-02,\n",
      "          3.0748e-02,  2.1146e-02,  1.4542e-02,  9.9998e-03,  6.8765e-03,\n",
      "          4.7287e-03,  3.2517e-03,  2.2361e-03,  1.5376e-03,  1.0574e-03,\n",
      "          7.2711e-04,  5.0000e-04,  3.4383e-04,  2.3644e-04,  1.6259e-04,\n",
      "          1.1180e-04,  7.6882e-05,  5.2869e-05,  3.6355e-05],\n",
      "        [-9.5892e-01, -2.9235e-01,  7.0131e-01,  9.9848e-01,  8.9924e-01,\n",
      "          6.9529e-01,  5.0440e-01,  3.5560e-01,  2.4740e-01,  1.7107e-01,\n",
      "          1.1794e-01,  8.1204e-02,  5.5873e-02,  3.8432e-02,  2.6431e-02,\n",
      "          1.8177e-02,  1.2500e-02,  8.5956e-03,  5.9109e-03,  4.0646e-03,\n",
      "          2.7951e-03,  1.9221e-03,  1.3217e-03,  9.0888e-04,  6.2500e-04,\n",
      "          4.2978e-04,  2.9554e-04,  2.0323e-04,  1.3975e-04,  9.6103e-05,\n",
      "          6.6086e-05,  4.5444e-05, -9.5892e-01, -2.9235e-01,  7.0131e-01,\n",
      "          9.9848e-01,  8.9924e-01,  6.9529e-01,  5.0440e-01,  3.5560e-01,\n",
      "          2.4740e-01,  1.7107e-01,  1.1794e-01,  8.1204e-02,  5.5873e-02,\n",
      "          3.8432e-02,  2.6431e-02,  1.8177e-02,  1.2500e-02,  8.5956e-03,\n",
      "          5.9109e-03,  4.0646e-03,  2.7951e-03,  1.9221e-03,  1.3217e-03,\n",
      "          9.0888e-04,  6.2500e-04,  4.2978e-04,  2.9554e-04,  2.0323e-04,\n",
      "          1.3975e-04,  9.6103e-05,  6.6086e-05,  4.5444e-05]])\n",
      "rope_cos shape after indexing: torch.Size([1, 11, 1, 64]), rope_sin shape after indexing: torch.Size([1, 11, 1, 64])\n",
      "rope_cos after indexing: tensor([[[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]],\n",
      "\n",
      "         [[ 0.5403,  0.7727,  0.8903,  0.9476,  0.9751,  0.9882,  0.9944,\n",
      "            0.9974,  0.9988,  0.9994,  0.9997,  0.9999,  0.9999,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  0.5403,  0.7727,  0.8903,\n",
      "            0.9476,  0.9751,  0.9882,  0.9944,  0.9974,  0.9988,  0.9994,\n",
      "            0.9997,  0.9999,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]],\n",
      "\n",
      "         [[-0.4161,  0.1942,  0.5851,  0.7959,  0.9017,  0.9531,  0.9777,\n",
      "            0.9894,  0.9950,  0.9976,  0.9989,  0.9995,  0.9998,  0.9999,\n",
      "            0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000, -0.4161,  0.1942,  0.5851,\n",
      "            0.7959,  0.9017,  0.9531,  0.9777,  0.9894,  0.9950,  0.9976,\n",
      "            0.9989,  0.9995,  0.9998,  0.9999,  0.9999,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]],\n",
      "\n",
      "         [[-0.9900, -0.4725,  0.1516,  0.5607,  0.7833,  0.8955,  0.9501,\n",
      "            0.9763,  0.9888,  0.9947,  0.9975,  0.9988,  0.9994,  0.9997,\n",
      "            0.9999,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000, -0.9900, -0.4725,  0.1516,\n",
      "            0.5607,  0.7833,  0.8955,  0.9501,  0.9763,  0.9888,  0.9947,\n",
      "            0.9975,  0.9988,  0.9994,  0.9997,  0.9999,  0.9999,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]],\n",
      "\n",
      "         [[-0.6536, -0.9245, -0.3152,  0.2668,  0.6260,  0.8167,  0.9119,\n",
      "            0.9580,  0.9801,  0.9906,  0.9955,  0.9979,  0.9990,  0.9995,\n",
      "            0.9998,  0.9999,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000, -0.6536, -0.9245, -0.3152,\n",
      "            0.2668,  0.6260,  0.8167,  0.9119,  0.9580,  0.9801,  0.9906,\n",
      "            0.9955,  0.9979,  0.9990,  0.9995,  0.9998,  0.9999,  0.9999,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]],\n",
      "\n",
      "         [[ 0.2837, -0.9563, -0.7129, -0.0550,  0.4375,  0.7187,  0.8635,\n",
      "            0.9346,  0.9689,  0.9853,  0.9930,  0.9967,  0.9984,  0.9993,\n",
      "            0.9997,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  0.2837, -0.9563, -0.7129,\n",
      "           -0.0550,  0.4375,  0.7187,  0.8635,  0.9346,  0.9689,  0.9853,\n",
      "            0.9930,  0.9967,  0.9984,  0.9993,  0.9997,  0.9998,  0.9999,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]],\n",
      "\n",
      "         [[ 0.5403,  0.7727,  0.8903,  0.9476,  0.9751,  0.9882,  0.9944,\n",
      "            0.9974,  0.9988,  0.9994,  0.9997,  0.9999,  0.9999,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  0.5403,  0.7727,  0.8903,\n",
      "            0.9476,  0.9751,  0.9882,  0.9944,  0.9974,  0.9988,  0.9994,\n",
      "            0.9997,  0.9999,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]],\n",
      "\n",
      "         [[-0.4161,  0.1942,  0.5851,  0.7959,  0.9017,  0.9531,  0.9777,\n",
      "            0.9894,  0.9950,  0.9976,  0.9989,  0.9995,  0.9998,  0.9999,\n",
      "            0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000, -0.4161,  0.1942,  0.5851,\n",
      "            0.7959,  0.9017,  0.9531,  0.9777,  0.9894,  0.9950,  0.9976,\n",
      "            0.9989,  0.9995,  0.9998,  0.9999,  0.9999,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]],\n",
      "\n",
      "         [[-0.9900, -0.4725,  0.1516,  0.5607,  0.7833,  0.8955,  0.9501,\n",
      "            0.9763,  0.9888,  0.9947,  0.9975,  0.9988,  0.9994,  0.9997,\n",
      "            0.9999,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000, -0.9900, -0.4725,  0.1516,\n",
      "            0.5607,  0.7833,  0.8955,  0.9501,  0.9763,  0.9888,  0.9947,\n",
      "            0.9975,  0.9988,  0.9994,  0.9997,  0.9999,  0.9999,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]],\n",
      "\n",
      "         [[-0.6536, -0.9245, -0.3152,  0.2668,  0.6260,  0.8167,  0.9119,\n",
      "            0.9580,  0.9801,  0.9906,  0.9955,  0.9979,  0.9990,  0.9995,\n",
      "            0.9998,  0.9999,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000, -0.6536, -0.9245, -0.3152,\n",
      "            0.2668,  0.6260,  0.8167,  0.9119,  0.9580,  0.9801,  0.9906,\n",
      "            0.9955,  0.9979,  0.9990,  0.9995,  0.9998,  0.9999,  0.9999,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "            1.0000]]]]), rope_sin after indexing: tensor([[[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 8.4147e-01,  6.3473e-01,  4.5544e-01,  3.1947e-01,  2.2175e-01,\n",
      "            1.5316e-01,  1.0554e-01,  7.2647e-02,  4.9979e-02,  3.4376e-02,\n",
      "            2.3641e-02,  1.6258e-02,  1.1180e-02,  7.6882e-03,  5.2868e-03,\n",
      "            3.6355e-03,  2.5000e-03,  1.7191e-03,  1.1822e-03,  8.1293e-04,\n",
      "            5.5902e-04,  3.8441e-04,  2.6434e-04,  1.8178e-04,  1.2500e-04,\n",
      "            8.5957e-05,  5.9109e-05,  4.0647e-05,  2.7951e-05,  1.9221e-05,\n",
      "            1.3217e-05,  9.0888e-06,  8.4147e-01,  6.3473e-01,  4.5544e-01,\n",
      "            3.1947e-01,  2.2175e-01,  1.5316e-01,  1.0554e-01,  7.2647e-02,\n",
      "            4.9979e-02,  3.4376e-02,  2.3641e-02,  1.6258e-02,  1.1180e-02,\n",
      "            7.6882e-03,  5.2868e-03,  3.6355e-03,  2.5000e-03,  1.7191e-03,\n",
      "            1.1822e-03,  8.1293e-04,  5.5902e-04,  3.8441e-04,  2.6434e-04,\n",
      "            1.8178e-04,  1.2500e-04,  8.5957e-05,  5.9109e-05,  4.0647e-05,\n",
      "            2.7951e-05,  1.9221e-05,  1.3217e-05,  9.0888e-06]],\n",
      "\n",
      "         [[ 9.0930e-01,  9.8095e-01,  8.1093e-01,  6.0546e-01,  4.3245e-01,\n",
      "            3.0270e-01,  2.0990e-01,  1.4491e-01,  9.9833e-02,  6.8711e-02,\n",
      "            4.7269e-02,  3.2512e-02,  2.2359e-02,  1.5376e-02,  1.0574e-02,\n",
      "            7.2710e-03,  5.0000e-03,  3.4383e-03,  2.3644e-03,  1.6259e-03,\n",
      "            1.1180e-03,  7.6882e-04,  5.2869e-04,  3.6355e-04,  2.5000e-04,\n",
      "            1.7191e-04,  1.1822e-04,  8.1293e-05,  5.5902e-05,  3.8441e-05,\n",
      "            2.6434e-05,  1.8178e-05,  9.0930e-01,  9.8095e-01,  8.1093e-01,\n",
      "            6.0546e-01,  4.3245e-01,  3.0270e-01,  2.0990e-01,  1.4491e-01,\n",
      "            9.9833e-02,  6.8711e-02,  4.7269e-02,  3.2512e-02,  2.2359e-02,\n",
      "            1.5376e-02,  1.0574e-02,  7.2710e-03,  5.0000e-03,  3.4383e-03,\n",
      "            2.3644e-03,  1.6259e-03,  1.1180e-03,  7.6882e-04,  5.2869e-04,\n",
      "            3.6355e-04,  2.5000e-04,  1.7191e-04,  1.1822e-04,  8.1293e-05,\n",
      "            5.5902e-05,  3.8441e-05,  2.6434e-05,  1.8178e-05]],\n",
      "\n",
      "         [[ 1.4112e-01,  8.8131e-01,  9.8844e-01,  8.2799e-01,  6.2163e-01,\n",
      "            4.4511e-01,  3.1192e-01,  2.1641e-01,  1.4944e-01,  1.0297e-01,\n",
      "            7.0871e-02,  4.8757e-02,  3.3535e-02,  2.3063e-02,  1.5860e-02,\n",
      "            1.0906e-02,  7.4999e-03,  5.1574e-03,  3.5465e-03,  2.4388e-03,\n",
      "            1.6771e-03,  1.1532e-03,  7.9303e-04,  5.4533e-04,  3.7500e-04,\n",
      "            2.5787e-04,  1.7733e-04,  1.2194e-04,  8.3853e-05,  5.7662e-05,\n",
      "            3.9651e-05,  2.7267e-05,  1.4112e-01,  8.8131e-01,  9.8844e-01,\n",
      "            8.2799e-01,  6.2163e-01,  4.4511e-01,  3.1192e-01,  2.1641e-01,\n",
      "            1.4944e-01,  1.0297e-01,  7.0871e-02,  4.8757e-02,  3.3535e-02,\n",
      "            2.3063e-02,  1.5860e-02,  1.0906e-02,  7.4999e-03,  5.1574e-03,\n",
      "            3.5465e-03,  2.4388e-03,  1.6771e-03,  1.1532e-03,  7.9303e-04,\n",
      "            5.4533e-04,  3.7500e-04,  2.5787e-04,  1.7733e-04,  1.2194e-04,\n",
      "            8.3853e-05,  5.7662e-05,  3.9651e-05,  2.7267e-05]],\n",
      "\n",
      "         [[-7.5680e-01,  3.8108e-01,  9.4902e-01,  9.6374e-01,  7.7985e-01,\n",
      "            5.7701e-01,  4.1045e-01,  2.8676e-01,  1.9867e-01,  1.3710e-01,\n",
      "            9.4433e-02,  6.4989e-02,  4.4706e-02,  3.0748e-02,  2.1146e-02,\n",
      "            1.4542e-02,  9.9998e-03,  6.8765e-03,  4.7287e-03,  3.2517e-03,\n",
      "            2.2361e-03,  1.5376e-03,  1.0574e-03,  7.2711e-04,  5.0000e-04,\n",
      "            3.4383e-04,  2.3644e-04,  1.6259e-04,  1.1180e-04,  7.6882e-05,\n",
      "            5.2869e-05,  3.6355e-05, -7.5680e-01,  3.8108e-01,  9.4902e-01,\n",
      "            9.6374e-01,  7.7985e-01,  5.7701e-01,  4.1045e-01,  2.8676e-01,\n",
      "            1.9867e-01,  1.3710e-01,  9.4433e-02,  6.4989e-02,  4.4706e-02,\n",
      "            3.0748e-02,  2.1146e-02,  1.4542e-02,  9.9998e-03,  6.8765e-03,\n",
      "            4.7287e-03,  3.2517e-03,  2.2361e-03,  1.5376e-03,  1.0574e-03,\n",
      "            7.2711e-04,  5.0000e-04,  3.4383e-04,  2.3644e-04,  1.6259e-04,\n",
      "            1.1180e-04,  7.6882e-05,  5.2869e-05,  3.6355e-05]],\n",
      "\n",
      "         [[-9.5892e-01, -2.9235e-01,  7.0131e-01,  9.9848e-01,  8.9924e-01,\n",
      "            6.9529e-01,  5.0440e-01,  3.5560e-01,  2.4740e-01,  1.7107e-01,\n",
      "            1.1794e-01,  8.1204e-02,  5.5873e-02,  3.8432e-02,  2.6431e-02,\n",
      "            1.8177e-02,  1.2500e-02,  8.5956e-03,  5.9109e-03,  4.0646e-03,\n",
      "            2.7951e-03,  1.9221e-03,  1.3217e-03,  9.0888e-04,  6.2500e-04,\n",
      "            4.2978e-04,  2.9554e-04,  2.0323e-04,  1.3975e-04,  9.6103e-05,\n",
      "            6.6086e-05,  4.5444e-05, -9.5892e-01, -2.9235e-01,  7.0131e-01,\n",
      "            9.9848e-01,  8.9924e-01,  6.9529e-01,  5.0440e-01,  3.5560e-01,\n",
      "            2.4740e-01,  1.7107e-01,  1.1794e-01,  8.1204e-02,  5.5873e-02,\n",
      "            3.8432e-02,  2.6431e-02,  1.8177e-02,  1.2500e-02,  8.5956e-03,\n",
      "            5.9109e-03,  4.0646e-03,  2.7951e-03,  1.9221e-03,  1.3217e-03,\n",
      "            9.0888e-04,  6.2500e-04,  4.2978e-04,  2.9554e-04,  2.0323e-04,\n",
      "            1.3975e-04,  9.6103e-05,  6.6086e-05,  4.5444e-05]],\n",
      "\n",
      "         [[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "            0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "         [[ 8.4147e-01,  6.3473e-01,  4.5544e-01,  3.1947e-01,  2.2175e-01,\n",
      "            1.5316e-01,  1.0554e-01,  7.2647e-02,  4.9979e-02,  3.4376e-02,\n",
      "            2.3641e-02,  1.6258e-02,  1.1180e-02,  7.6882e-03,  5.2868e-03,\n",
      "            3.6355e-03,  2.5000e-03,  1.7191e-03,  1.1822e-03,  8.1293e-04,\n",
      "            5.5902e-04,  3.8441e-04,  2.6434e-04,  1.8178e-04,  1.2500e-04,\n",
      "            8.5957e-05,  5.9109e-05,  4.0647e-05,  2.7951e-05,  1.9221e-05,\n",
      "            1.3217e-05,  9.0888e-06,  8.4147e-01,  6.3473e-01,  4.5544e-01,\n",
      "            3.1947e-01,  2.2175e-01,  1.5316e-01,  1.0554e-01,  7.2647e-02,\n",
      "            4.9979e-02,  3.4376e-02,  2.3641e-02,  1.6258e-02,  1.1180e-02,\n",
      "            7.6882e-03,  5.2868e-03,  3.6355e-03,  2.5000e-03,  1.7191e-03,\n",
      "            1.1822e-03,  8.1293e-04,  5.5902e-04,  3.8441e-04,  2.6434e-04,\n",
      "            1.8178e-04,  1.2500e-04,  8.5957e-05,  5.9109e-05,  4.0647e-05,\n",
      "            2.7951e-05,  1.9221e-05,  1.3217e-05,  9.0888e-06]],\n",
      "\n",
      "         [[ 9.0930e-01,  9.8095e-01,  8.1093e-01,  6.0546e-01,  4.3245e-01,\n",
      "            3.0270e-01,  2.0990e-01,  1.4491e-01,  9.9833e-02,  6.8711e-02,\n",
      "            4.7269e-02,  3.2512e-02,  2.2359e-02,  1.5376e-02,  1.0574e-02,\n",
      "            7.2710e-03,  5.0000e-03,  3.4383e-03,  2.3644e-03,  1.6259e-03,\n",
      "            1.1180e-03,  7.6882e-04,  5.2869e-04,  3.6355e-04,  2.5000e-04,\n",
      "            1.7191e-04,  1.1822e-04,  8.1293e-05,  5.5902e-05,  3.8441e-05,\n",
      "            2.6434e-05,  1.8178e-05,  9.0930e-01,  9.8095e-01,  8.1093e-01,\n",
      "            6.0546e-01,  4.3245e-01,  3.0270e-01,  2.0990e-01,  1.4491e-01,\n",
      "            9.9833e-02,  6.8711e-02,  4.7269e-02,  3.2512e-02,  2.2359e-02,\n",
      "            1.5376e-02,  1.0574e-02,  7.2710e-03,  5.0000e-03,  3.4383e-03,\n",
      "            2.3644e-03,  1.6259e-03,  1.1180e-03,  7.6882e-04,  5.2869e-04,\n",
      "            3.6355e-04,  2.5000e-04,  1.7191e-04,  1.1822e-04,  8.1293e-05,\n",
      "            5.5902e-05,  3.8441e-05,  2.6434e-05,  1.8178e-05]],\n",
      "\n",
      "         [[ 1.4112e-01,  8.8131e-01,  9.8844e-01,  8.2799e-01,  6.2163e-01,\n",
      "            4.4511e-01,  3.1192e-01,  2.1641e-01,  1.4944e-01,  1.0297e-01,\n",
      "            7.0871e-02,  4.8757e-02,  3.3535e-02,  2.3063e-02,  1.5860e-02,\n",
      "            1.0906e-02,  7.4999e-03,  5.1574e-03,  3.5465e-03,  2.4388e-03,\n",
      "            1.6771e-03,  1.1532e-03,  7.9303e-04,  5.4533e-04,  3.7500e-04,\n",
      "            2.5787e-04,  1.7733e-04,  1.2194e-04,  8.3853e-05,  5.7662e-05,\n",
      "            3.9651e-05,  2.7267e-05,  1.4112e-01,  8.8131e-01,  9.8844e-01,\n",
      "            8.2799e-01,  6.2163e-01,  4.4511e-01,  3.1192e-01,  2.1641e-01,\n",
      "            1.4944e-01,  1.0297e-01,  7.0871e-02,  4.8757e-02,  3.3535e-02,\n",
      "            2.3063e-02,  1.5860e-02,  1.0906e-02,  7.4999e-03,  5.1574e-03,\n",
      "            3.5465e-03,  2.4388e-03,  1.6771e-03,  1.1532e-03,  7.9303e-04,\n",
      "            5.4533e-04,  3.7500e-04,  2.5787e-04,  1.7733e-04,  1.2194e-04,\n",
      "            8.3853e-05,  5.7662e-05,  3.9651e-05,  2.7267e-05]],\n",
      "\n",
      "         [[-7.5680e-01,  3.8108e-01,  9.4902e-01,  9.6374e-01,  7.7985e-01,\n",
      "            5.7701e-01,  4.1045e-01,  2.8676e-01,  1.9867e-01,  1.3710e-01,\n",
      "            9.4433e-02,  6.4989e-02,  4.4706e-02,  3.0748e-02,  2.1146e-02,\n",
      "            1.4542e-02,  9.9998e-03,  6.8765e-03,  4.7287e-03,  3.2517e-03,\n",
      "            2.2361e-03,  1.5376e-03,  1.0574e-03,  7.2711e-04,  5.0000e-04,\n",
      "            3.4383e-04,  2.3644e-04,  1.6259e-04,  1.1180e-04,  7.6882e-05,\n",
      "            5.2869e-05,  3.6355e-05, -7.5680e-01,  3.8108e-01,  9.4902e-01,\n",
      "            9.6374e-01,  7.7985e-01,  5.7701e-01,  4.1045e-01,  2.8676e-01,\n",
      "            1.9867e-01,  1.3710e-01,  9.4433e-02,  6.4989e-02,  4.4706e-02,\n",
      "            3.0748e-02,  2.1146e-02,  1.4542e-02,  9.9998e-03,  6.8765e-03,\n",
      "            4.7287e-03,  3.2517e-03,  2.2361e-03,  1.5376e-03,  1.0574e-03,\n",
      "            7.2711e-04,  5.0000e-04,  3.4383e-04,  2.3644e-04,  1.6259e-04,\n",
      "            1.1180e-04,  7.6882e-05,  5.2869e-05,  3.6355e-05]]]])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "rope: torch.Size([1, 11, 1, 64]), torch.Size([1, 11, 1, 64])\n",
      "rope q k shape: torch.Size([1, 11, 12, 64]), torch.Size([1, 11, 12, 64])\n",
      "Unpadding sequence_output\n",
      "sequence_output: torch.Size([1, 11, 768]), indices: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), batch_size: 2, seq_length: 6\n",
      "sequence_output after unpadding: torch.Size([2, 6, 768])\n",
      "Document 1: A blue cat.\n",
      "Embedding: [-3.22074555e-02  2.57213488e-02 -6.34935498e-02  4.25290242e-02\n",
      " -1.26504079e-02 -3.68123017e-02 -2.07916703e-02  2.90931240e-02\n",
      "  2.46046279e-02  6.48620129e-02 -1.18698822e-02 -2.26637386e-02\n",
      " -1.33556738e-01  2.02255733e-02  2.26927754e-02  4.62176949e-02\n",
      "  4.16552573e-02 -4.51662242e-02 -1.47883117e-03 -5.26436418e-02\n",
      "  6.39320388e-02 -3.41061614e-02 -6.83119753e-03 -2.48586331e-02\n",
      " -5.18326536e-02  7.84089416e-02 -4.61502746e-02 -2.88600475e-02\n",
      "  3.77100259e-02 -1.83354272e-03 -7.32841715e-02 -2.46344022e-02\n",
      " -1.69608165e-02 -1.26726292e-02  3.61169167e-02 -2.85730232e-03\n",
      " -5.49945906e-02 -6.20641299e-02  2.11889092e-02  6.86033303e-03\n",
      " -2.58740727e-02 -3.47631350e-02 -2.47447845e-02 -1.40197331e-03\n",
      "  3.48953903e-03  3.41836326e-02 -2.95952354e-02  1.52111575e-02\n",
      " -3.32418345e-02 -1.72988325e-02 -3.07614934e-02  5.53587899e-02\n",
      "  1.42982136e-03  3.40021402e-02 -2.97032502e-02 -3.81729286e-03\n",
      " -1.13796517e-02  1.97411831e-02  5.63993603e-02 -1.03380745e-02\n",
      " -4.01369557e-02 -1.40123712e-02 -4.38265540e-02 -2.73635425e-03\n",
      "  5.35506494e-02 -1.47343278e-02 -2.60723438e-02 -2.45831329e-02\n",
      "  6.11414313e-02  2.69433260e-02 -5.62671535e-02  4.47617937e-03\n",
      " -7.80437365e-02 -1.13819605e-02  6.95662275e-02 -6.25983160e-03\n",
      "  8.46497423e-04  6.91682752e-03  2.19440516e-02 -5.36069926e-03\n",
      " -2.90619452e-02 -1.66512057e-02  3.27277035e-02  2.13972814e-02\n",
      " -7.00189844e-02  1.45893334e-03 -6.88142702e-02 -5.37548028e-02\n",
      "  2.13482436e-02  5.42616360e-02  7.42368773e-03  1.16956225e-02\n",
      "  2.20398931e-03 -2.78885979e-02  2.57411543e-02 -4.94531728e-02\n",
      " -6.89853579e-02 -3.44446786e-02  2.25166287e-02 -6.94276718e-03\n",
      " -2.08294131e-02 -1.28342407e-02  3.35365161e-02  2.55556796e-02\n",
      "  2.48313267e-02 -3.41749415e-02  1.80121250e-02  1.50044309e-03\n",
      " -1.30967796e-02 -1.12748714e-02 -6.86102081e-03  5.17432019e-03\n",
      " -1.60580513e-03 -4.33542719e-03 -8.30477774e-02  4.43011373e-02\n",
      "  5.13444059e-02 -8.49506184e-02  6.66090054e-03  7.08615333e-02\n",
      " -2.57195849e-02 -3.33186015e-02  3.12536210e-02  2.54118517e-02\n",
      " -1.38277963e-01 -1.37034608e-02 -4.80332226e-02 -1.35197295e-02\n",
      " -4.14747074e-02 -5.09976111e-02  7.98172355e-02  1.90066826e-02\n",
      " -2.27814037e-02 -5.18880710e-02  2.83404924e-02 -4.46230248e-02\n",
      " -1.66438892e-02  7.87729695e-02 -1.43080177e-02 -2.01308429e-02\n",
      " -1.02080107e-01  1.31018497e-02 -9.07563418e-02  3.16913705e-03\n",
      " -5.96750947e-03 -2.63005607e-02 -4.50655445e-02 -4.84128892e-02\n",
      "  2.62957010e-02  3.50161381e-02  2.74605043e-02  2.19109803e-02\n",
      "  2.95480117e-02 -1.81238260e-02 -1.97628736e-02 -6.98094349e-03\n",
      " -7.72008346e-03  8.51177722e-02 -1.08068939e-02  3.93195376e-02\n",
      "  4.78751250e-02 -1.14647942e-02 -3.20221670e-03 -6.14684913e-03\n",
      "  2.09812932e-02 -2.59485748e-02 -7.90740028e-02  5.04920110e-02\n",
      "  9.20087099e-02  5.89348413e-02 -1.29635751e-01  1.33444741e-02\n",
      "  3.58466133e-02  6.02678731e-02 -4.13745679e-02  2.28653513e-02\n",
      "  3.76129174e-03  3.64525281e-02 -8.90956894e-02 -3.66688631e-02\n",
      " -4.18012328e-02 -8.95918719e-03  2.85865515e-02  2.73282323e-02\n",
      " -4.40912647e-03  1.76164694e-02 -6.12068884e-02  9.11971461e-03\n",
      "  9.72712785e-03  2.90941298e-02 -4.72458266e-02 -1.07635334e-02\n",
      " -7.81969577e-02 -6.56951731e-03  2.18306147e-02 -7.57400692e-02\n",
      "  1.95283610e-02  4.67031486e-02  7.41260592e-03  5.47057092e-02\n",
      "  1.44330887e-02 -5.05446680e-02 -2.04268377e-02  1.19843269e-02\n",
      " -3.22321281e-02 -3.89388506e-03 -3.37457173e-02 -4.65240479e-02\n",
      "  1.14523908e-02 -1.40678257e-01  7.55175622e-03  5.70762642e-02\n",
      " -1.10680675e-02  5.64179663e-03  2.84953937e-02  1.57364234e-02\n",
      " -4.31858674e-02  1.46089382e-02  2.75686588e-02  4.94913124e-02\n",
      " -1.64243933e-02 -3.21672782e-02 -6.32034522e-03  2.84731761e-02\n",
      "  5.46911433e-02  7.23636104e-03 -1.65301915e-02 -1.50062079e-02\n",
      "  4.61028852e-02 -2.56057587e-02  4.46470417e-02 -8.05773959e-03\n",
      "  2.97647640e-02  3.05157937e-02  1.72899365e-02  5.39253578e-02\n",
      " -2.56039817e-02 -4.57606502e-02 -2.31231228e-02  8.19391757e-02\n",
      "  2.07910165e-02 -1.92238931e-02  3.46104130e-02  3.28900516e-02\n",
      "  5.00345007e-02  5.34750447e-02 -4.17003892e-02  4.28004824e-02\n",
      "  4.02559116e-02 -2.67410595e-02 -2.57095136e-02  2.72555016e-02\n",
      " -4.36541922e-02 -4.52056602e-02 -2.25302554e-03 -8.60770121e-02\n",
      " -6.47788541e-03 -1.70294140e-02  3.80829046e-03  5.80980889e-02\n",
      " -6.13075942e-02  1.75970495e-02  4.20207419e-02 -2.25443617e-02\n",
      " -1.63801778e-02  1.39261521e-02  9.22889709e-02  6.08683378e-02\n",
      " -3.34453350e-03  4.83722202e-02 -3.01276166e-02  6.56519830e-02\n",
      "  2.22856805e-04  4.97444421e-02  2.52753999e-02 -1.93811376e-02\n",
      " -2.40482390e-02 -4.15517986e-02 -4.69544120e-02  1.27246520e-02\n",
      "  8.31085630e-03 -1.02054495e-02 -2.80397665e-02  6.87298030e-02\n",
      "  8.88136309e-03  2.74819322e-02 -2.42395364e-02 -2.12029871e-02\n",
      " -2.05169269e-03  1.79819446e-02  1.62440445e-03  2.93401107e-02\n",
      " -1.60083417e-02 -1.02705369e-02  3.39311212e-02  3.42920683e-02\n",
      " -1.32057080e-02 -1.78606994e-03  2.89728260e-03  5.34749292e-02\n",
      " -3.44311185e-02 -6.05404228e-02  1.07570142e-02 -2.19470393e-02\n",
      "  5.22472616e-03  5.09926043e-02 -1.96020324e-02 -5.97364753e-02\n",
      " -1.49951642e-02 -1.42572597e-02 -3.11222542e-02 -7.02300109e-03\n",
      "  4.30460609e-02 -2.84437165e-02  3.86828072e-02 -7.06425160e-02\n",
      "  1.00155007e-02  6.81794807e-02 -1.70017052e-02 -3.05110179e-02\n",
      " -1.03209801e-02 -1.37082394e-02 -4.40021083e-02  2.83732880e-02\n",
      " -4.41179201e-02 -1.51328025e-02  9.04679764e-03 -6.51988247e-03\n",
      " -1.13078486e-02  6.98341010e-03  2.59534959e-02  4.21329252e-02\n",
      "  6.68766201e-02 -2.97232736e-02  5.24536036e-02 -6.66953027e-02\n",
      " -1.29809929e-02 -6.95814891e-03  2.85337716e-02  2.84261070e-02\n",
      "  3.90946306e-02  1.41482083e-02 -4.86582480e-02 -1.60729811e-02\n",
      "  2.07894966e-02 -4.45399899e-03  1.48705777e-03 -5.06108031e-02\n",
      "  1.73226167e-02 -5.00071123e-02 -4.67212349e-02  1.54494224e-02\n",
      " -2.92817522e-02  2.84225680e-02  2.05475129e-02 -1.03463894e-02\n",
      "  3.13316137e-02 -1.54846313e-03 -4.48487066e-02 -1.18812490e-02\n",
      "  5.93872974e-03  2.69637406e-02  9.66590550e-03  5.45503609e-02\n",
      "  5.04029430e-02  2.46838704e-02  4.73714992e-03  4.37933244e-02\n",
      " -2.54048640e-03  7.56776845e-03 -2.39814427e-02  7.36033125e-03\n",
      "  9.91573930e-03 -4.61133523e-03 -4.05433476e-02  2.05807928e-02\n",
      " -2.43040100e-02 -9.78699047e-03 -1.50137972e-02 -1.92868302e-03\n",
      "  4.23999429e-02 -1.42127124e-03  4.53104302e-02 -1.32137751e-02\n",
      " -2.53871724e-04 -6.23582341e-02  1.60869397e-02 -8.46518576e-03\n",
      " -1.82538349e-02 -1.32800927e-02  4.63716723e-02 -3.57876765e-03\n",
      " -5.70231229e-02  2.42251735e-02  7.60696530e-02  1.05988234e-02\n",
      "  3.85175496e-02 -1.99037269e-02 -2.32319217e-02 -1.55115807e-02\n",
      "  1.60144959e-02  5.22511005e-02  4.27149702e-03  2.03285227e-03\n",
      " -3.25442180e-02  1.21404240e-02 -4.41488024e-04  1.43929170e-02\n",
      "  3.90495220e-03 -3.18098180e-02  3.86012383e-02 -2.90000308e-02\n",
      "  2.23908871e-02 -2.94840392e-02 -5.45645365e-03  3.90234888e-02\n",
      " -3.49416077e-04 -2.56957877e-02  7.87126347e-02 -3.15920152e-02\n",
      " -1.42406230e-03  3.59685719e-02  2.96026934e-02 -2.75015645e-02\n",
      " -2.14150678e-02 -1.36639895e-02  5.09366067e-03  6.08395366e-03\n",
      " -2.93858983e-02  7.68954027e-03 -4.22138646e-02 -1.06535517e-02\n",
      " -6.54316470e-02 -6.14833944e-02 -7.30358902e-03 -1.42367072e-02\n",
      "  3.10533214e-03  3.01702395e-02 -3.64812016e-02 -2.60479599e-02\n",
      " -3.30079570e-02 -3.43581941e-03  2.80836620e-03 -2.86903400e-02\n",
      "  1.73478127e-02 -2.03416130e-05 -4.44588624e-02  1.07988212e-02\n",
      "  2.02556774e-02  5.24799190e-02 -6.43510967e-02  2.60960078e-03\n",
      "  1.08802384e-02  1.61193218e-02 -4.44657020e-02  1.36161726e-02\n",
      " -1.36199994e-02  1.22284843e-02 -2.51370552e-03  4.08961661e-02\n",
      "  1.59031302e-02  4.31192815e-02  3.36922682e-03  3.99841741e-02\n",
      "  2.12229658e-02  5.23490794e-02 -4.25243238e-03  5.78751462e-03\n",
      "  4.28835973e-02  7.44990213e-03 -4.42055278e-02 -3.19148824e-02\n",
      " -4.49504331e-02 -1.01677990e-02 -3.40174325e-02  4.04785201e-02\n",
      "  1.68734416e-02  1.04232663e-02 -3.47618572e-02  1.68715399e-02\n",
      "  4.53008898e-02  6.26021549e-02  4.12096381e-02 -1.40793789e-02\n",
      " -1.47394463e-02 -1.29741319e-02  1.25721053e-04 -2.65385620e-02\n",
      "  6.75832853e-03  5.39669655e-02  7.52482796e-03  2.00954676e-02\n",
      "  3.54480445e-02 -2.92173736e-02  2.18613148e-02 -2.75284960e-03\n",
      " -1.06991166e-02  4.60370407e-02  1.07027022e-02 -2.56253388e-02\n",
      " -2.54457816e-04  1.83961540e-02 -2.11012317e-03 -3.90045205e-03\n",
      " -3.28396144e-03 -3.26513797e-02  2.00465303e-02  2.42727417e-02\n",
      "  5.72108338e-03 -1.80401802e-02 -1.19621539e-02 -2.49990411e-02\n",
      "  4.36608344e-02 -4.13451195e-02 -3.37560698e-02 -2.88540553e-02\n",
      " -3.64234895e-02  2.16577630e-02 -1.45614836e-02 -2.49661487e-02\n",
      " -2.35187970e-02  3.43781635e-02 -1.89968720e-02  6.06683120e-02\n",
      "  2.72062160e-02 -2.79380046e-02  4.18408252e-02  2.13293508e-02\n",
      "  3.86626311e-02 -1.40425917e-02  4.70808856e-02  2.02261414e-02\n",
      "  2.48188712e-02  2.90395459e-03  1.93842538e-02  5.11739738e-02\n",
      " -6.74275123e-03  1.99324545e-02 -4.30789962e-02 -1.52341221e-02\n",
      " -3.85352448e-02 -3.44629698e-02 -1.10701742e-02 -4.26839851e-02\n",
      "  1.59009360e-02  4.94561791e-02  7.28568435e-02 -3.41600217e-02\n",
      " -4.49430756e-03 -4.45494466e-02  1.81900132e-02 -5.86279086e-04\n",
      " -1.96547862e-02  5.08396514e-02 -6.17251135e-02  1.15589742e-02\n",
      "  1.63656771e-02  2.22433172e-02  1.66053064e-02  3.22096609e-02\n",
      " -1.87394861e-02  2.43428312e-02  3.71451154e-02 -4.86573242e-02\n",
      " -3.51017006e-02  1.49053438e-02  1.23562934e-02 -9.61859711e-04\n",
      "  5.06029911e-02  3.09648998e-02  5.80183230e-02 -3.55735272e-02\n",
      "  2.97181271e-02 -4.03612852e-03  5.64907305e-02 -2.75616460e-02\n",
      " -6.33310825e-02 -3.57569531e-02 -1.47423698e-02  1.05629629e-02\n",
      " -8.72907788e-02 -1.42807569e-02  2.75218096e-02  3.87882115e-03\n",
      "  1.17426254e-02  4.99337949e-02 -3.72310802e-02  1.79615933e-02\n",
      " -2.82142032e-03 -1.13742184e-02 -1.00737568e-02  3.29196006e-02\n",
      "  4.14407067e-02  4.07112390e-03  2.56611742e-02 -6.17518649e-03\n",
      "  3.54963839e-02 -2.66134106e-02  5.08437166e-03 -1.48871150e-02\n",
      "  1.30104041e-03 -1.54164033e-02  5.29270666e-03 -7.19925994e-03\n",
      " -7.75247440e-03 -3.46691981e-02 -1.30927693e-02 -1.44827766e-02\n",
      " -1.53837688e-02  1.28118899e-02 -3.40713821e-02 -2.40808297e-02\n",
      "  1.97218712e-02 -1.68263875e-02  4.73372601e-02 -2.64371689e-02\n",
      " -2.66105253e-02  1.35682710e-02  8.97202361e-03  5.34070795e-03\n",
      " -1.31468028e-02 -4.52532843e-02  4.26123776e-02  2.04180516e-02\n",
      "  1.38660818e-02 -4.60070334e-02  4.10257503e-02 -5.92815783e-03\n",
      "  8.96500598e-04  1.07019674e-02 -1.05995946e-02  9.78572108e-03\n",
      " -9.72664729e-03 -2.71462332e-02  4.90937196e-03  3.05793546e-02\n",
      "  7.29039498e-03 -2.42034462e-03  1.79393906e-02  2.96443305e-03\n",
      "  2.63750162e-02  5.05271181e-02  3.89250070e-02 -4.57346216e-02\n",
      " -5.17867915e-02 -7.46640516e-03  3.08031254e-02 -2.96176672e-02\n",
      " -7.18725324e-02  8.39449540e-02  5.11147156e-02  5.40448129e-02\n",
      "  1.75786763e-02  5.50329033e-03 -5.25129633e-03 -2.68850476e-02\n",
      " -3.53162326e-02  7.74439983e-03 -1.39197463e-03  1.02196865e-01\n",
      "  2.59788204e-02  3.85605870e-03 -8.77944846e-03 -2.72986516e-02\n",
      " -1.72053222e-02  1.39224743e-02  2.91070230e-02  4.01827693e-02\n",
      "  4.66760583e-02  4.08216454e-02 -1.69252884e-02  3.68443653e-02\n",
      "  4.73546460e-02  1.21741928e-02 -5.48314974e-02 -1.16535788e-02\n",
      " -4.43888493e-02 -4.38203290e-02  6.77802274e-03 -2.78003179e-02\n",
      "  4.22868468e-02  1.05263088e-02  1.26373628e-02 -3.05528734e-02\n",
      " -3.07379123e-02 -7.37610832e-02  6.07911944e-02 -1.48832183e-02\n",
      "  2.37545604e-03  3.16849649e-02 -5.61698480e-03  1.91670489e-02\n",
      "  6.58052340e-02 -7.81278685e-02  6.65486902e-02 -2.78032161e-02\n",
      " -2.59495135e-02 -1.44303720e-02  2.62510944e-02 -4.18192223e-02\n",
      " -4.08573002e-02 -1.60023011e-02  2.57817935e-02  4.03495831e-03\n",
      " -2.88270358e-02  1.77283045e-02  2.46011671e-02  9.97799914e-03\n",
      " -2.42796075e-02  4.07195976e-03  2.35325359e-02  2.00630687e-02\n",
      " -2.48737093e-02  2.73750629e-02  1.99135691e-02  1.75071955e-02\n",
      "  3.49684469e-02 -3.51734683e-02 -2.73730829e-02  2.12881267e-01\n",
      "  2.11396012e-02  1.65587422e-02  2.37013157e-02  2.64295191e-03\n",
      "  2.06043348e-02  2.72301603e-02  2.78006848e-02  5.61891589e-03\n",
      " -2.57343464e-02 -2.80785691e-02  1.01195266e-02  2.70071737e-02\n",
      "  6.67023612e-03  2.24151853e-02  1.13523137e-02 -4.36521508e-02\n",
      "  1.78520959e-02  5.25896028e-02 -3.89128812e-02 -4.21577580e-02\n",
      "  2.56764814e-02 -1.53500102e-02 -1.37209855e-02 -1.79619808e-02\n",
      "  4.87485155e-03  8.60490091e-03  7.26230517e-02 -2.51308717e-02\n",
      "  1.46186631e-02 -8.06250842e-04 -2.28859801e-02 -4.84811747e-03\n",
      "  9.95805115e-03  1.43457139e-02  6.74383668e-03 -2.82552149e-02\n",
      "  2.79876087e-02 -2.55702920e-02  3.75216193e-02 -3.94178219e-02\n",
      "  4.05174904e-02 -6.94088498e-03  6.83349976e-03  1.25474911e-02\n",
      "  2.70237750e-03 -4.28652801e-02 -2.51204893e-02 -6.14777878e-02]\n",
      "\n",
      "Document 2: A blue cat\n",
      "Embedding: [-4.37088907e-02  2.43543331e-02 -7.39331916e-02  4.05579060e-02\n",
      " -1.80413630e-02 -3.35013568e-02 -2.56192889e-02  3.63119952e-02\n",
      "  3.08795329e-02  5.94892837e-02 -1.77343749e-02 -3.08900084e-02\n",
      " -1.19969167e-01  1.11098997e-02  7.12975254e-03  4.86099906e-02\n",
      "  4.09523360e-02 -2.60163322e-02  1.54911298e-02 -4.48166914e-02\n",
      "  9.60970521e-02 -3.00353672e-02 -5.51803689e-03 -3.18316324e-03\n",
      " -4.58059311e-02  6.32778183e-02 -4.30311412e-02 -2.53690723e-02\n",
      "  3.28915790e-02 -2.61492804e-02 -7.25693479e-02 -4.75339554e-02\n",
      " -2.72440668e-02 -2.53808703e-02  3.23743969e-02  9.14597046e-03\n",
      " -3.50532569e-02 -6.04485907e-02  2.99425647e-02  7.79716298e-03\n",
      " -4.31877375e-02 -4.42339256e-02 -2.62414366e-02  8.77665915e-03\n",
      "  6.17683399e-03  2.62577590e-02 -4.23055477e-02  4.28507812e-02\n",
      " -1.76152289e-02 -1.85957029e-02 -4.28638123e-02  5.63585758e-02\n",
      "  6.27823081e-03  2.84445621e-02 -6.26323139e-03  1.27175832e-02\n",
      " -2.20532399e-02  6.75349217e-03  5.79504408e-02 -1.09031349e-02\n",
      " -5.18029742e-02 -9.29741003e-03 -4.61389050e-02 -3.67794360e-04\n",
      "  2.33127382e-02 -1.09410891e-02 -2.01308820e-02 -2.37005576e-02\n",
      "  7.62178078e-02  4.12378870e-02 -4.91462536e-02  1.89573914e-02\n",
      " -7.85549507e-02  5.79663832e-03  7.19606280e-02 -5.60553232e-03\n",
      " -1.40646463e-02 -9.71599296e-03  1.78288259e-02 -2.28511300e-02\n",
      " -3.16943713e-02 -2.56335046e-02  2.68179737e-02  6.09413302e-03\n",
      " -5.28417043e-02  1.52073670e-02 -6.50106072e-02 -5.66343591e-02\n",
      "  2.14745998e-02  4.72032018e-02 -3.73868877e-03  2.73306910e-02\n",
      " -1.45016508e-02 -2.18949355e-02  2.97538750e-02 -5.64661212e-02\n",
      " -6.33481517e-02 -3.27703990e-02  1.04217548e-02 -8.37221835e-03\n",
      "  9.82873328e-03 -4.91616316e-03  2.56744791e-02  1.55636957e-02\n",
      "  1.51897920e-02 -4.96342219e-02  2.02954523e-02  8.45474249e-04\n",
      "  1.44340126e-02  1.37448870e-02 -1.48134865e-03  3.86423105e-03\n",
      " -2.47332901e-02  9.78838187e-03 -6.17344864e-02  3.94187644e-02\n",
      "  6.69166595e-02 -8.01803544e-02 -8.13768536e-04  6.16195574e-02\n",
      " -2.21710075e-02 -3.04024369e-02  3.85434553e-02  1.85097121e-02\n",
      " -1.38122901e-01 -1.55922500e-02 -5.49220406e-02 -1.09589929e-02\n",
      " -2.62621455e-02 -4.99050766e-02  9.28588510e-02  5.47693670e-03\n",
      " -9.33502987e-03 -5.81181087e-02  4.09926549e-02 -4.33052219e-02\n",
      " -2.81044226e-02  7.39595965e-02 -1.46838585e-02 -1.92487873e-02\n",
      " -1.09896563e-01  1.12222005e-02 -8.92670453e-02  8.70849472e-03\n",
      " -2.30951756e-02 -2.96360217e-02 -3.90620716e-02 -3.86134945e-02\n",
      "  2.00486481e-02  3.28889899e-02  2.19119340e-02  2.72270516e-02\n",
      "  4.40101698e-02 -2.34248247e-02 -2.09844168e-02 -2.55399235e-02\n",
      " -7.04667717e-03  7.66758323e-02  2.03295494e-03  3.89790870e-02\n",
      "  4.04329970e-02 -2.46855943e-03  8.88004247e-03 -1.13761341e-02\n",
      "  5.46871778e-03 -4.88046482e-02 -6.26767948e-02  2.90814769e-02\n",
      "  8.79706889e-02  7.36969560e-02 -1.25312492e-01  1.39401183e-02\n",
      "  4.40984517e-02  7.42627382e-02 -2.26638932e-02  1.58801395e-02\n",
      " -2.51706387e-03  1.49942879e-02 -9.63230133e-02 -3.91731188e-02\n",
      " -3.29406038e-02  1.15434006e-02  1.91938560e-02  1.81257185e-02\n",
      "  2.64066877e-03  1.20162070e-02 -5.62325642e-02  1.43191312e-02\n",
      " -2.23375717e-03  2.19670255e-02 -4.27705236e-02 -2.25827377e-03\n",
      " -1.01711199e-01  1.23173520e-02  2.39105113e-02 -5.75127751e-02\n",
      "  2.33406126e-02  6.17492422e-02  1.81452953e-03  5.30523770e-02\n",
      "  2.20849719e-02 -5.46490997e-02 -2.32940298e-02  4.92840074e-03\n",
      " -3.84071320e-02 -8.74575973e-03 -3.85704823e-02 -5.48488982e-02\n",
      " -8.04235321e-03 -1.42435625e-01  6.66141976e-03  5.59914634e-02\n",
      " -2.12682243e-02 -4.97298643e-05  1.23980343e-02  2.52047088e-02\n",
      " -7.10685998e-02  6.48858305e-03  3.84396054e-02  3.24383117e-02\n",
      " -8.03141575e-03 -1.83041003e-02 -2.21896730e-02  3.47840823e-02\n",
      "  5.62122948e-02  7.11920531e-03 -1.23937353e-02 -9.96338669e-03\n",
      "  6.14452772e-02 -4.26644310e-02  4.47321795e-02 -1.29714487e-02\n",
      "  1.77109689e-02  2.93742791e-02  1.05415741e-02  4.11569774e-02\n",
      " -2.15507224e-02 -6.44436106e-02 -1.41289309e-02  6.28313869e-02\n",
      "  2.81593669e-02 -2.81038899e-02  3.30806524e-02  4.75878678e-02\n",
      "  5.77217564e-02  5.04676104e-02 -3.48302685e-02  8.74524005e-03\n",
      "  4.97391783e-02 -3.06627974e-02 -6.11206563e-03  1.85851473e-02\n",
      " -3.39523144e-02 -4.54197563e-02 -1.80910702e-03 -7.91146681e-02\n",
      " -1.56918429e-02 -2.77406015e-02 -2.41738968e-02  6.45356849e-02\n",
      " -5.80656081e-02  7.63488002e-03  2.32484639e-02 -5.63916238e-03\n",
      " -3.61986384e-02  1.65533200e-02  7.77323097e-02  6.19597249e-02\n",
      " -1.90426596e-02  3.82149480e-02 -1.97314937e-02  6.45957813e-02\n",
      "  2.29898840e-03  5.48409857e-02  2.43488085e-02  4.35786461e-03\n",
      " -2.89817322e-02 -3.12605426e-02 -3.79461236e-02  1.10233231e-02\n",
      "  2.42314879e-02 -1.61306967e-03 -3.06255389e-02  5.56532405e-02\n",
      "  1.24372272e-02  1.94284134e-02 -1.53144654e-02 -4.41118442e-02\n",
      " -1.25274733e-02  1.92653593e-02  2.46407371e-03  2.86633875e-02\n",
      " -3.33584175e-02 -2.27687182e-03  4.91923094e-02  2.89724879e-02\n",
      " -2.62742136e-02  1.57603305e-02  2.15867646e-02  5.52714728e-02\n",
      " -2.41346061e-02 -7.00830817e-02  1.86886992e-02 -2.74007265e-02\n",
      "  2.56168190e-03  5.92483617e-02 -2.68439781e-02 -5.46837859e-02\n",
      " -4.11140779e-03 -1.41125312e-02 -3.77140008e-02 -1.68668907e-02\n",
      "  3.99599634e-02 -2.88170334e-02  3.11254151e-02 -5.45979217e-02\n",
      "  3.60802077e-02  8.21747482e-02 -2.62021963e-02 -2.36431304e-02\n",
      " -3.00131179e-02 -4.63114260e-03 -5.01714312e-02  4.45533432e-02\n",
      " -4.81644385e-02 -2.01218389e-02  2.04430409e-02  1.99576952e-02\n",
      " -2.73531452e-02 -2.64333241e-04  2.55976822e-02  2.32337583e-02\n",
      "  5.65061234e-02 -2.58046705e-02  5.23345172e-02 -5.32648899e-02\n",
      " -1.95237976e-02 -5.26745897e-03  2.41315868e-02  1.64294243e-02\n",
      "  9.84648988e-03  2.57134158e-02 -4.70241755e-02 -7.61679932e-03\n",
      "  6.85976353e-03  5.17988997e-03  1.65720973e-02 -4.00538258e-02\n",
      "  5.02747251e-03 -3.57029699e-02 -2.76677441e-02  1.94984581e-02\n",
      " -2.61220429e-02  3.75476852e-02  3.96462008e-02 -1.97836664e-02\n",
      "  2.18152404e-02 -1.32880537e-02 -5.64642213e-02 -1.08562969e-02\n",
      "  1.36817908e-02  1.17865698e-02  4.19963291e-03  4.48666960e-02\n",
      "  4.12179641e-02  1.41968131e-02  1.66137870e-02  3.28723565e-02\n",
      " -1.51497964e-02 -1.10591780e-02 -2.80619338e-02 -9.22584441e-03\n",
      " -1.01163732e-02  6.66798383e-04 -3.97501141e-02  2.06366219e-02\n",
      " -2.05753613e-02 -2.05257572e-02 -2.54821051e-02  8.53899680e-03\n",
      "  4.54207547e-02  1.89076969e-03  4.51714657e-02 -2.19979603e-02\n",
      "  6.56278105e-03 -4.16299216e-02  4.39081760e-03 -5.25074033e-03\n",
      " -7.06938747e-03 -3.17866844e-03  5.91411777e-02  4.53864317e-03\n",
      " -5.53106852e-02  2.59747263e-02  7.79978335e-02  1.20306425e-02\n",
      "  2.28106380e-02 -2.17388254e-02 -2.81709433e-02 -1.72169637e-02\n",
      "  2.69551072e-02  3.60901728e-02  2.28116801e-03  1.69204059e-03\n",
      " -1.86228398e-02  1.91766676e-02 -3.97187518e-03  3.81865306e-03\n",
      " -7.89126195e-03 -4.50956486e-02  2.39951648e-02 -2.72161346e-02\n",
      "  1.79325137e-02 -2.93854307e-02 -4.46435483e-03  3.59619670e-02\n",
      " -6.10112865e-03 -8.36799387e-03  1.02495521e-01 -2.90908068e-02\n",
      "  1.40442606e-02  2.66324189e-02  1.87971815e-02 -5.89306885e-03\n",
      " -1.14412261e-02 -1.11015327e-02 -2.19908683e-03 -1.65123469e-03\n",
      " -3.75064202e-02  1.32841459e-02 -4.91403937e-02 -7.96586648e-03\n",
      " -7.07639679e-02 -5.64682595e-02  1.70405710e-03 -1.51580516e-02\n",
      "  2.24317443e-02  2.46882718e-02 -5.17610237e-02 -3.71635146e-02\n",
      " -2.24751942e-02  2.40662834e-03 -1.29366992e-04 -2.55484954e-02\n",
      "  2.07439475e-02 -1.66037702e-03 -2.74868123e-02  5.04875183e-03\n",
      "  1.63757745e-02  4.12838124e-02 -5.79516962e-02  1.60449762e-02\n",
      "  1.54878115e-02  2.73788255e-02 -4.51002941e-02  2.68988907e-02\n",
      " -4.93797287e-03  1.18561294e-02  1.33484071e-02  3.03217303e-02\n",
      "  4.94777178e-03  1.38919065e-02  9.23285261e-03  2.67688818e-02\n",
      "  7.07213301e-03  4.37872186e-02 -7.75684975e-03  3.03798472e-03\n",
      "  1.48154665e-02  1.02083227e-02 -4.12532091e-02 -2.98382919e-02\n",
      " -4.02992889e-02 -1.56901628e-02 -2.10988782e-02  3.54935229e-02\n",
      "  2.07683705e-02  4.47153067e-03 -3.24048512e-02  1.42594716e-02\n",
      "  2.97227055e-02  5.74503280e-02  3.07652112e-02 -6.59308303e-03\n",
      " -2.29220577e-02 -7.26456614e-03 -1.30319195e-02 -2.63830610e-02\n",
      "  1.51042407e-02  6.42908886e-02  3.28626409e-02  2.91761383e-02\n",
      "  2.50878986e-02 -3.82406563e-02  3.55087928e-02 -8.35974701e-03\n",
      " -6.00060541e-03  4.49886359e-02  2.86820829e-02 -1.45240240e-02\n",
      "  2.28809682e-03  1.73746645e-02 -5.97830955e-03  1.22324424e-02\n",
      "  4.27222112e-03 -3.14330235e-02  9.52559244e-03  2.70318538e-02\n",
      "  5.19855507e-03  1.34804389e-02 -1.38619700e-02 -3.50810448e-03\n",
      "  4.84203473e-02 -3.61095890e-02 -4.17329594e-02 -3.18439007e-02\n",
      " -3.55824530e-02  2.54273657e-02 -1.08295437e-02 -2.43999362e-02\n",
      " -2.52663717e-02  3.22190300e-02 -1.55268330e-02  6.91593364e-02\n",
      "  4.40346859e-02 -4.94298041e-02  2.84474343e-02  3.16659771e-02\n",
      "  2.10245568e-02 -1.50497565e-02  5.16088083e-02  2.78876200e-02\n",
      "  2.05869395e-02  9.18346923e-03  3.42628844e-02  5.19839600e-02\n",
      "  1.97431399e-03  3.76691073e-02 -4.22318801e-02 -3.18962261e-02\n",
      " -3.09868529e-02 -1.99619923e-02 -3.01029962e-02 -3.65195163e-02\n",
      "  2.52844952e-03  5.47757968e-02  9.59346890e-02 -5.53286150e-02\n",
      "  6.67138398e-03 -3.57213467e-02 -1.90739101e-03 -3.56102153e-03\n",
      " -3.43112424e-02  4.76033799e-02 -3.25345509e-02 -1.22377148e-03\n",
      "  8.39394890e-03  2.56974231e-02  2.08876897e-02  3.45042013e-02\n",
      " -1.08835781e-02  1.61256511e-02  1.70705151e-02 -3.58711705e-02\n",
      " -5.20597585e-02  2.22071223e-02  1.51134431e-02 -1.39872767e-02\n",
      "  4.54541519e-02  3.65357921e-02  4.93285358e-02 -3.43482904e-02\n",
      "  3.65969762e-02 -1.90756738e-03  4.27936502e-02 -1.92979258e-02\n",
      " -4.34757844e-02 -2.86089312e-02 -1.15312031e-02  3.19738612e-02\n",
      " -8.38565156e-02 -3.60098407e-02  2.66715959e-02  1.29346661e-02\n",
      "  1.83415171e-02  4.86299321e-02 -5.09961247e-02  1.65587687e-03\n",
      "  8.56408384e-03 -1.14334235e-02 -1.02055902e-02  4.16488089e-02\n",
      "  2.89932396e-02  2.54174054e-04  3.68797630e-02 -1.24395906e-03\n",
      "  2.11381931e-02 -3.74669731e-02  1.74764041e-02 -1.86710954e-02\n",
      " -2.86355591e-03 -3.17495465e-02  6.01971289e-04 -1.46311028e-02\n",
      " -5.20105101e-03 -1.49920722e-02 -1.05707617e-02 -1.39186187e-02\n",
      " -3.20376903e-02  9.00998618e-03 -3.15259621e-02 -2.70858780e-02\n",
      "  2.22161934e-02 -7.13399239e-03  5.01971245e-02 -3.91424969e-02\n",
      " -2.33813357e-02  1.35512184e-02  9.03629046e-03  9.80696175e-03\n",
      " -9.73375048e-03 -4.45680618e-02  4.06421050e-02  2.12219823e-02\n",
      "  8.37115850e-03 -5.69943413e-02  4.76767048e-02 -1.09421685e-02\n",
      " -5.29986527e-03  5.16652549e-03 -4.29185759e-03  3.75985801e-02\n",
      " -6.70472905e-03 -1.87750533e-02  2.72902772e-02  2.47689728e-02\n",
      " -1.39256404e-03 -1.38086108e-02  6.15747739e-03  1.04371747e-02\n",
      "  3.47784273e-02  2.93823592e-02  1.92915071e-02 -3.57253104e-02\n",
      " -5.61443567e-02  4.80020838e-03  3.28585170e-02 -3.70186903e-02\n",
      " -4.87631522e-02  6.09597713e-02  5.86502105e-02  5.74998781e-02\n",
      "  3.12922033e-03  1.87947266e-02 -6.35431265e-04 -2.76531354e-02\n",
      " -4.35284823e-02  5.25081006e-04  3.81771801e-03  9.76023450e-02\n",
      "  3.83786904e-03  5.98793291e-03  1.90620916e-03 -3.11860740e-02\n",
      " -1.81530397e-02  3.23234871e-02  2.41765403e-03  1.51839601e-02\n",
      "  3.43622938e-02  3.60704549e-02 -9.18285176e-03  3.06480527e-02\n",
      "  4.52748798e-02  3.02765239e-03 -6.53541237e-02  4.85201646e-03\n",
      " -3.09078898e-02 -3.10462620e-02  2.74131820e-02 -4.63890620e-02\n",
      "  2.79204138e-02  8.61079153e-03  1.30801070e-02 -2.09808350e-02\n",
      " -2.69198250e-02 -8.38338807e-02  5.11898212e-02 -1.19532049e-02\n",
      "  8.92225839e-03  3.19264196e-02 -3.41727072e-03  2.45842319e-02\n",
      "  5.01176529e-02 -6.88929930e-02  7.25969970e-02 -4.68429923e-02\n",
      " -2.41641235e-02  1.51025801e-04  1.92963444e-02 -5.06289899e-02\n",
      " -1.91897731e-02 -1.62509419e-02  1.79455038e-02  4.77761263e-03\n",
      " -3.52276973e-02  1.56204100e-03  1.89788397e-02  1.88181270e-02\n",
      " -1.76898222e-02 -7.92735722e-03  2.17917636e-02  2.09494289e-02\n",
      " -2.53389832e-02  1.77139472e-02  1.88408028e-02  2.42848266e-02\n",
      "  3.56308408e-02 -4.95837294e-02 -2.20390521e-02  2.43662432e-01\n",
      "  1.09168682e-02  2.25543436e-02  2.11216193e-02  1.34298122e-02\n",
      "  1.24117536e-02  2.60199029e-02  3.15263756e-02  5.32834232e-03\n",
      " -3.09697352e-02 -2.91809514e-02  1.28199039e-02  2.04299130e-02\n",
      "  2.44597923e-02  2.85292435e-02  2.07444813e-04 -4.14302051e-02\n",
      "  1.40572637e-02  4.34340686e-02 -2.34667286e-02 -2.31716130e-02\n",
      "  2.62955930e-02  1.25581473e-02 -2.90457029e-02 -2.42066244e-03\n",
      "  2.26830374e-02  5.67791145e-03  6.90974593e-02 -1.98405795e-02\n",
      "  2.97757350e-02 -1.75830559e-04 -8.26085452e-03  5.29120024e-03\n",
      "  2.70406855e-03  9.84202884e-03  1.18022263e-02 -4.46178243e-02\n",
      "  3.05896215e-02 -3.97037864e-02  3.16647328e-02 -2.21464559e-02\n",
      "  3.66530754e-02 -4.62439610e-03  1.86487585e-02  1.37067745e-02\n",
      "  7.64078181e-03 -3.70973125e-02 -1.52757810e-02 -6.21692687e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:2080\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:2080\"\n",
    "model_name = 'Snowflake/snowflake-arctic-embed-m-v2.0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, add_pooling_layer=False, trust_remote_code=True, use_memory_efficient_attention=False)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "documents = ['A blue cat.', 'A blue cat', 'A cat']\n",
    "document_tokens =  tokenizer(documents, padding=True, truncation=True, return_tensors='pt', max_length=8192)\n",
    "\n",
    "print(document_tokens)\n",
    "\n",
    "# Move inputs to same device as model\n",
    "document_tokens = {k: v.to(device) for k, v in document_tokens.items()}\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    document_embeddings = model(**document_tokens)[0][:, 0]\n",
    "\n",
    "\n",
    "# normalize embeddings\n",
    "document_embeddings = torch.nn.functional.normalize(document_embeddings, p=2, dim=1)\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "    print(f\"Embedding: {document_embeddings[i].cpu().numpy()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb4fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.37089577e-02  2.43543386e-02 -7.39330947e-02  4.05579172e-02\n",
      "  -1.80412754e-02 -3.35012674e-02 -2.56192908e-02  3.63120325e-02\n",
      "   3.08795143e-02  5.94893135e-02 -1.77344810e-02 -3.08901053e-02\n",
      "  -1.19969159e-01  1.11098858e-02  7.12967850e-03  4.86099496e-02\n",
      "   4.09523770e-02 -2.60164179e-02  1.54911261e-02 -4.48167212e-02\n",
      "   9.60970595e-02 -3.00353728e-02 -5.51805412e-03 -3.18316813e-03\n",
      "  -4.58059460e-02  6.32777736e-02 -4.30310741e-02 -2.53691040e-02\n",
      "   3.28916237e-02 -2.61493474e-02 -7.25693554e-02 -4.75339927e-02\n",
      "  -2.72440091e-02 -2.53808890e-02  3.23742777e-02  9.14607570e-03\n",
      "  -3.50533575e-02 -6.04486093e-02  2.99426056e-02  7.79717788e-03\n",
      "  -4.31877784e-02 -4.42339294e-02 -2.62413621e-02  8.77672341e-03\n",
      "   6.17679255e-03  2.62576751e-02 -4.23056409e-02  4.28507663e-02\n",
      "  -1.76152233e-02 -1.85956955e-02 -4.28638048e-02  5.63584790e-02\n",
      "   6.27824292e-03  2.84444727e-02 -6.26334362e-03  1.27175646e-02\n",
      "  -2.20532417e-02  6.75354898e-03  5.79505190e-02 -1.09032188e-02\n",
      "  -5.18030412e-02 -9.29736346e-03 -4.61390167e-02 -3.67714354e-04\n",
      "   2.33127326e-02 -1.09411152e-02 -2.01308560e-02 -2.37006117e-02\n",
      "   7.62179121e-02  4.12379503e-02 -4.91462126e-02  1.89573988e-02\n",
      "  -7.85548910e-02  5.79665741e-03  7.19606131e-02 -5.60543966e-03\n",
      "  -1.40646705e-02 -9.71593056e-03  1.78287048e-02 -2.28510723e-02\n",
      "  -3.16943154e-02 -2.56334972e-02  2.68179402e-02  6.09407295e-03\n",
      "  -5.28416671e-02  1.52073056e-02 -6.50105253e-02 -5.66342957e-02\n",
      "   2.14745700e-02  4.72031720e-02 -3.73854488e-03  2.73307394e-02\n",
      "  -1.45016918e-02 -2.18949579e-02  2.97539495e-02 -5.64660989e-02\n",
      "  -6.33481592e-02 -3.27704810e-02  1.04217092e-02 -8.37222859e-03\n",
      "   9.82876960e-03 -4.91619948e-03  2.56744511e-02  1.55637097e-02\n",
      "   1.51897967e-02 -4.96342853e-02  2.02954970e-02  8.45508999e-04\n",
      "   1.44340647e-02  1.37449484e-02 -1.48137775e-03  3.86421522e-03\n",
      "  -2.47333087e-02  9.78842750e-03 -6.17344454e-02  3.94187793e-02\n",
      "   6.69166148e-02 -8.01803321e-02 -8.13780935e-04  6.16196059e-02\n",
      "  -2.21710894e-02 -3.04024648e-02  3.85434628e-02  1.85097642e-02\n",
      "  -1.38122827e-01 -1.55923450e-02 -5.49220219e-02 -1.09589268e-02\n",
      "  -2.62621325e-02 -4.99049835e-02  9.28589031e-02  5.47692319e-03\n",
      "  -9.33496654e-03 -5.81180342e-02  4.09926213e-02 -4.33053225e-02\n",
      "  -2.81043947e-02  7.39596486e-02 -1.46839013e-02 -1.92487594e-02\n",
      "  -1.09896548e-01  1.12221912e-02 -8.92670453e-02  8.70849751e-03\n",
      "  -2.30951943e-02 -2.96360105e-02 -3.90621163e-02 -3.86133790e-02\n",
      "   2.00485103e-02  3.28890160e-02  2.19120104e-02  2.72270683e-02\n",
      "   4.40102741e-02 -2.34248657e-02 -2.09843758e-02 -2.55399253e-02\n",
      "  -7.04669766e-03  7.66758174e-02  2.03295960e-03  3.89790647e-02\n",
      "   4.04329523e-02 -2.46860320e-03  8.88001733e-03 -1.13761108e-02\n",
      "   5.46875736e-03 -4.88045402e-02 -6.26767874e-02  2.90814023e-02\n",
      "   8.79707262e-02  7.36969337e-02 -1.25312537e-01  1.39400531e-02\n",
      "   4.40984778e-02  7.42626712e-02 -2.26638298e-02  1.58800650e-02\n",
      "  -2.51705130e-03  1.49942655e-02 -9.63229313e-02 -3.91731299e-02\n",
      "  -3.29406150e-02  1.15434611e-02  1.91939846e-02  1.81257557e-02\n",
      "   2.64073582e-03  1.20162154e-02 -5.62324934e-02  1.43191759e-02\n",
      "  -2.23369990e-03  2.19669677e-02 -4.27705720e-02 -2.25819601e-03\n",
      "  -1.01711228e-01  1.23173427e-02  2.39105709e-02 -5.75126708e-02\n",
      "   2.33406592e-02  6.17492385e-02  1.81447342e-03  5.30523472e-02\n",
      "   2.20848769e-02 -5.46491556e-02 -2.32941583e-02  4.92834626e-03\n",
      "  -3.84070985e-02 -8.74579512e-03 -3.85705531e-02 -5.48488908e-02\n",
      "  -8.04244727e-03 -1.42435551e-01  6.66132802e-03  5.59914522e-02\n",
      "  -2.12681312e-02 -4.96230023e-05  1.23979179e-02  2.52046715e-02\n",
      "  -7.10686371e-02  6.48843916e-03  3.84396277e-02  3.24382745e-02\n",
      "  -8.03127140e-03 -1.83041934e-02 -2.21897271e-02  3.47840749e-02\n",
      "   5.62123433e-02  7.11921323e-03 -1.23938294e-02 -9.96342860e-03\n",
      "   6.14452139e-02 -4.26642597e-02  4.47322428e-02 -1.29715065e-02\n",
      "   1.77109502e-02  2.93742791e-02  1.05416495e-02  4.11570072e-02\n",
      "  -2.15507206e-02 -6.44436851e-02 -1.41289188e-02  6.28313869e-02\n",
      "   2.81593855e-02 -2.81038247e-02  3.30807343e-02  4.75879461e-02\n",
      "   5.77217489e-02  5.04676104e-02 -3.48302275e-02  8.74524843e-03\n",
      "   4.97391932e-02 -3.06628793e-02 -6.11203536e-03  1.85852684e-02\n",
      "  -3.39523517e-02 -4.54198234e-02 -1.80906733e-03 -7.91146606e-02\n",
      "  -1.56917870e-02 -2.77406145e-02 -2.41739322e-02  6.45357519e-02\n",
      "  -5.80656752e-02  7.63488375e-03  2.32484471e-02 -5.63915307e-03\n",
      "  -3.61987017e-02  1.65533274e-02  7.77322352e-02  6.19597472e-02\n",
      "  -1.90426912e-02  3.82148810e-02 -1.97315253e-02  6.45957738e-02\n",
      "   2.29900610e-03  5.48410788e-02  2.43487656e-02  4.35794191e-03\n",
      "  -2.89818030e-02 -3.12605612e-02 -3.79461236e-02  1.10234087e-02\n",
      "   2.42316294e-02 -1.61319890e-03 -3.06256209e-02  5.56532778e-02\n",
      "   1.24370819e-02  1.94283631e-02 -1.53144458e-02 -4.41118516e-02\n",
      "  -1.25275198e-02  1.92653369e-02  2.46413145e-03  2.86633931e-02\n",
      "  -3.33584175e-02 -2.27683666e-03  4.91922684e-02  2.89725848e-02\n",
      "  -2.62742862e-02  1.57603361e-02  2.15868391e-02  5.52714169e-02\n",
      "  -2.41345484e-02 -7.00831190e-02  1.86888222e-02 -2.74006464e-02\n",
      "   2.56166654e-03  5.92482835e-02 -2.68440023e-02 -5.46838380e-02\n",
      "  -4.11137566e-03 -1.41125470e-02 -3.77140120e-02 -1.68668628e-02\n",
      "   3.99599746e-02 -2.88170725e-02  3.11254375e-02 -5.45979068e-02\n",
      "   3.60802338e-02  8.21748227e-02 -2.62021255e-02 -2.36431696e-02\n",
      "  -3.00131403e-02 -4.63106483e-03 -5.01714498e-02  4.45532613e-02\n",
      "  -4.81644273e-02 -2.01218463e-02  2.04429906e-02  1.99577007e-02\n",
      "  -2.73530614e-02 -2.64312577e-04  2.55975798e-02  2.32337341e-02\n",
      "   5.65060489e-02 -2.58046649e-02  5.23344651e-02 -5.32649159e-02\n",
      "  -1.95237175e-02 -5.26744965e-03  2.41315253e-02  1.64295156e-02\n",
      "   9.84648429e-03  2.57134847e-02 -4.70242128e-02 -7.61668058e-03\n",
      "   6.85974723e-03  5.17992768e-03  1.65720508e-02 -4.00537290e-02\n",
      "   5.02749300e-03 -3.57029326e-02 -2.76677832e-02  1.94983855e-02\n",
      "  -2.61220317e-02  3.75476405e-02  3.96461822e-02 -1.97836850e-02\n",
      "   2.18152329e-02 -1.32879624e-02 -5.64643070e-02 -1.08562438e-02\n",
      "   1.36817498e-02  1.17865950e-02  4.19975817e-03  4.48667519e-02\n",
      "   4.12178785e-02  1.41967777e-02  1.66139286e-02  3.28724422e-02\n",
      "  -1.51498793e-02 -1.10591985e-02 -2.80618910e-02 -9.22583882e-03\n",
      "  -1.01165017e-02  6.66862528e-04 -3.97500545e-02  2.06366796e-02\n",
      "  -2.05754265e-02 -2.05256958e-02 -2.54819952e-02  8.53900146e-03\n",
      "   4.54206727e-02  1.89075491e-03  4.51715253e-02 -2.19980367e-02\n",
      "   6.56278664e-03 -4.16298620e-02  4.39082040e-03 -5.25073474e-03\n",
      "  -7.06949411e-03 -3.17872595e-03  5.91411553e-02  4.53862548e-03\n",
      "  -5.53107522e-02  2.59748194e-02  7.79978260e-02  1.20305493e-02\n",
      "   2.28107236e-02 -2.17387937e-02 -2.81709787e-02 -1.72170177e-02\n",
      "   2.69550681e-02  3.60901579e-02  2.28120503e-03  1.69202907e-03\n",
      "  -1.86227169e-02  1.91766862e-02 -3.97193152e-03  3.81859741e-03\n",
      "  -7.89123867e-03 -4.50956598e-02  2.39951350e-02 -2.72160266e-02\n",
      "   1.79324616e-02 -2.93852873e-02 -4.46445029e-03  3.59619521e-02\n",
      "  -6.10109139e-03 -8.36805161e-03  1.02495529e-01 -2.90907826e-02\n",
      "   1.40443016e-02  2.66324133e-02  1.87972132e-02 -5.89300925e-03\n",
      "  -1.14412354e-02 -1.11015365e-02 -2.19913153e-03 -1.65119546e-03\n",
      "  -3.75064388e-02  1.32840695e-02 -4.91403937e-02 -7.96583202e-03\n",
      "  -7.07639307e-02 -5.64682633e-02  1.70397328e-03 -1.51581103e-02\n",
      "   2.24317834e-02  2.46882848e-02 -5.17610386e-02 -3.71635668e-02\n",
      "  -2.24751849e-02  2.40679318e-03 -1.29420208e-04 -2.55485326e-02\n",
      "   2.07439009e-02 -1.66041346e-03 -2.74867751e-02  5.04876301e-03\n",
      "   1.63757354e-02  4.12838981e-02 -5.79516850e-02  1.60448346e-02\n",
      "   1.54878795e-02  2.73787118e-02 -4.51002307e-02  2.68987734e-02\n",
      "  -4.93800407e-03  1.18561238e-02  1.33484630e-02  3.03217340e-02\n",
      "   4.94774897e-03  1.38918972e-02  9.23281722e-03  2.67688911e-02\n",
      "   7.07206316e-03  4.37871628e-02 -7.75687303e-03  3.03804455e-03\n",
      "   1.48153566e-02  1.02082817e-02 -4.12531346e-02 -2.98382752e-02\n",
      "  -4.02993076e-02 -1.56901460e-02 -2.10988969e-02  3.54935043e-02\n",
      "   2.07684152e-02  4.47151531e-03 -3.24049294e-02  1.42594883e-02\n",
      "   2.97226794e-02  5.74503168e-02  3.07651795e-02 -6.59305649e-03\n",
      "  -2.29221135e-02 -7.26464577e-03 -1.30319744e-02 -2.63831150e-02\n",
      "   1.51042938e-02  6.42909184e-02  3.28626335e-02  2.91762296e-02\n",
      "   2.50878874e-02 -3.82406041e-02  3.55087519e-02 -8.35978147e-03\n",
      "  -6.00062404e-03  4.49885875e-02  2.86821797e-02 -1.45240752e-02\n",
      "   2.28807866e-03  1.73747633e-02 -5.97832538e-03  1.22324964e-02\n",
      "   4.27217502e-03 -3.14330272e-02  9.52549465e-03  2.70317867e-02\n",
      "   5.19855553e-03  1.34804491e-02 -1.38620464e-02 -3.50807491e-03\n",
      "   4.84204218e-02 -3.61095555e-02 -4.17329669e-02 -3.18438485e-02\n",
      "  -3.55824344e-02  2.54274644e-02 -1.08295856e-02 -2.43998617e-02\n",
      "  -2.52663326e-02  3.22191119e-02 -1.55268870e-02  6.91593438e-02\n",
      "   4.40347157e-02 -4.94297706e-02  2.84474473e-02  3.16659734e-02\n",
      "   2.10245699e-02 -1.50496950e-02  5.16088121e-02  2.78875716e-02\n",
      "   2.05870010e-02  9.18351393e-03  3.42628546e-02  5.19839339e-02\n",
      "   1.97438453e-03  3.76690738e-02 -4.22319286e-02 -3.18962336e-02\n",
      "  -3.09868529e-02 -1.99620035e-02 -3.01030148e-02 -3.65194902e-02\n",
      "   2.52843089e-03  5.47756925e-02  9.59346443e-02 -5.53287454e-02\n",
      "   6.67131832e-03 -3.57213542e-02 -1.90741674e-03 -3.56102875e-03\n",
      "  -3.43112424e-02  4.76033054e-02 -3.25344317e-02 -1.22375775e-03\n",
      "   8.39392655e-03  2.56974809e-02  2.08876971e-02  3.45042199e-02\n",
      "  -1.08835483e-02  1.61256287e-02  1.70704871e-02 -3.58711556e-02\n",
      "  -5.20597436e-02  2.22071111e-02  1.51134692e-02 -1.39872879e-02\n",
      "   4.54541557e-02  3.65357250e-02  4.93285507e-02 -3.43482792e-02\n",
      "   3.65970321e-02 -1.90757040e-03  4.27937545e-02 -1.92979332e-02\n",
      "  -4.34757546e-02 -2.86089387e-02 -1.15311118e-02  3.19738388e-02\n",
      "  -8.38565081e-02 -3.60099226e-02  2.66715884e-02  1.29347239e-02\n",
      "   1.83415934e-02  4.86299992e-02 -5.09962030e-02  1.65593508e-03\n",
      "   8.56407918e-03 -1.14335287e-02 -1.02055967e-02  4.16488163e-02\n",
      "   2.89932583e-02  2.54177023e-04  3.68797481e-02 -1.24402822e-03\n",
      "   2.11381763e-02 -3.74670699e-02  1.74764749e-02 -1.86710414e-02\n",
      "  -2.86359759e-03 -3.17496881e-02  6.01923501e-04 -1.46311549e-02\n",
      "  -5.20107662e-03 -1.49921058e-02 -1.05707161e-02 -1.39186438e-02\n",
      "  -3.20376903e-02  9.01003554e-03 -3.15259248e-02 -2.70858258e-02\n",
      "   2.22162176e-02 -7.13398308e-03  5.01970574e-02 -3.91424447e-02\n",
      "  -2.33813953e-02  1.35511803e-02  9.03631654e-03  9.80698876e-03\n",
      "  -9.73380171e-03 -4.45680842e-02  4.06421162e-02  2.12220792e-02\n",
      "   8.37119296e-03 -5.69944009e-02  4.76766750e-02 -1.09421536e-02\n",
      "  -5.29979868e-03  5.16656879e-03 -4.29190649e-03  3.75987180e-02\n",
      "  -6.70467597e-03 -1.87750086e-02  2.72902288e-02  2.47689746e-02\n",
      "  -1.39263493e-03 -1.38084991e-02  6.15747925e-03  1.04371617e-02\n",
      "   3.47784720e-02  2.93823797e-02  1.92914642e-02 -3.57251763e-02\n",
      "  -5.61443083e-02  4.80022747e-03  3.28584984e-02 -3.70187834e-02\n",
      "  -4.87632230e-02  6.09597415e-02  5.86502030e-02  5.74999824e-02\n",
      "   3.12919240e-03  1.87947284e-02 -6.35431963e-04 -2.76531037e-02\n",
      "  -4.35284674e-02  5.25163428e-04  3.81772500e-03  9.76023600e-02\n",
      "   3.83780268e-03  5.98803582e-03  1.90629624e-03 -3.11861020e-02\n",
      "  -1.81530360e-02  3.23234983e-02  2.41762423e-03  1.51838828e-02\n",
      "   3.43622491e-02  3.60704623e-02 -9.18279029e-03  3.06480788e-02\n",
      "   4.52749357e-02  3.02759442e-03 -6.53541163e-02  4.85199550e-03\n",
      "  -3.09080295e-02 -3.10462862e-02  2.74132155e-02 -4.63890247e-02\n",
      "   2.79203374e-02  8.61088652e-03  1.30801313e-02 -2.09807884e-02\n",
      "  -2.69197915e-02 -8.38339403e-02  5.11897579e-02 -1.19531574e-02\n",
      "   8.92235246e-03  3.19262259e-02 -3.41735827e-03  2.45841742e-02\n",
      "   5.01176380e-02 -6.88931122e-02  7.25969374e-02 -4.68429402e-02\n",
      "  -2.41640899e-02  1.51169079e-04  1.92963257e-02 -5.06290831e-02\n",
      "  -1.91896856e-02 -1.62508432e-02  1.79454349e-02  4.77759214e-03\n",
      "  -3.52277122e-02  1.56202866e-03  1.89788509e-02  1.88180506e-02\n",
      "  -1.76897999e-02 -7.92741962e-03  2.17917841e-02  2.09494177e-02\n",
      "  -2.53390837e-02  1.77139398e-02  1.88408531e-02  2.42848489e-02\n",
      "   3.56307961e-02 -4.95836362e-02 -2.20391247e-02  2.43662387e-01\n",
      "   1.09168049e-02  2.25543398e-02  2.11216658e-02  1.34297889e-02\n",
      "   1.24116885e-02  2.60200389e-02  3.15263905e-02  5.32840751e-03\n",
      "  -3.09696961e-02 -2.91809365e-02  1.28199095e-02  2.04299875e-02\n",
      "   2.44597644e-02  2.85292827e-02  2.07463323e-04 -4.14302982e-02\n",
      "   1.40573159e-02  4.34340797e-02 -2.34667510e-02 -2.31715348e-02\n",
      "   2.62956806e-02  1.25581110e-02 -2.90456917e-02 -2.42066500e-03\n",
      "   2.26829574e-02  5.67797013e-03  6.90974966e-02 -1.98405776e-02\n",
      "   2.97758374e-02 -1.75953101e-04 -8.26078095e-03  5.29116485e-03\n",
      "   2.70405272e-03  9.84208472e-03  1.18022766e-02 -4.46178615e-02\n",
      "   3.05895619e-02 -3.97037901e-02  3.16646621e-02 -2.21464317e-02\n",
      "   3.66530716e-02 -4.62435558e-03  1.86488032e-02  1.37067148e-02\n",
      "   7.64072686e-03 -3.70972939e-02 -1.52757736e-02 -6.21691793e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model_name = 'Snowflake/snowflake-arctic-embed-m-v2.0'\n",
    "model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "print(document_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings.cpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
