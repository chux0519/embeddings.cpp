{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7af4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from gguf import GGUFWriter, GGMLQuantizationType\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:2080\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:2080\"\n",
    "\n",
    "def convert_hf(repo_id, output_path, float_type='f16'):\n",
    "    # convert to ggml quantization type\n",
    "    if float_type not in ['f16', 'f32']:\n",
    "        print(f'Float type must be f16 or f32, got: {float_type}')\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        qtype = GGMLQuantizationType[float_type.upper()]\n",
    "        dtype0 = {'f16': torch.float16, 'f32': torch.float32}[float_type]\n",
    "\n",
    "    # load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "    tokenizer_json = os.path.join(os.path.dirname(output_path), os.path.basename(repo_id) + \".tokenizer.json\")\n",
    "    # tokenizer.save_pretrained(tokenizer_json)\n",
    "    tokenizer._tokenizer.save(tokenizer_json, False)\n",
    "    model = AutoModel.from_pretrained(repo_id, add_pooling_layer=False, trust_remote_code=True)\n",
    "\n",
    "    config = model.config\n",
    "    print(config)\n",
    "    \n",
    "    # print model\n",
    "    param_keys = [\n",
    "        'vocab_size', 'hidden_size', 'num_hidden_layers',\n",
    "        'num_attention_heads', 'intermediate_size', 'type_vocab_size', 'pad_token_id'\n",
    "    ]\n",
    "    print('PARAMS')\n",
    "    for k in param_keys:\n",
    "        v = getattr(config, k)\n",
    "        print(f'{k:<24s} = {v}')\n",
    "    print()\n",
    "\n",
    "    # print vocab\n",
    "    vocab_keys = [\n",
    "        'vocab_size', 'pad_token_id', 'unk_token_id', 'cls_token_id', 'sep_token_id'\n",
    "    ]\n",
    "    print('VOCAB')\n",
    "    for k in vocab_keys:\n",
    "        v = getattr(tokenizer, k)\n",
    "        print(f'{k:24s} = {v}')\n",
    "\n",
    "\n",
    "    # start to write GGUF file\n",
    "    gguf_writer = GGUFWriter(output_path, \"JinaBert\")\n",
    "\n",
    "    # write metadata\n",
    "    gguf_writer.add_name(repo_id)\n",
    "    gguf_writer.add_description('gguf model for embeddings.cpp')\n",
    "    gguf_writer.add_file_type(qtype)\n",
    "\n",
    "    # write model params\n",
    "    gguf_writer.add_uint32('vocab_size', config.vocab_size)\n",
    "    gguf_writer.add_uint32('hidden_size', config.hidden_size)\n",
    "    gguf_writer.add_uint32('intermediate_size', config.intermediate_size)\n",
    "    gguf_writer.add_uint32('num_attention_heads', config.num_attention_heads)\n",
    "    gguf_writer.add_uint32('num_hidden_layers', config.num_hidden_layers)\n",
    "    gguf_writer.add_uint32('type_vocab_size', config.type_vocab_size)\n",
    "    gguf_writer.add_uint32('pad_token_id', config.pad_token_id)\n",
    "    gguf_writer.add_float32('layer_norm_eps', config.layer_norm_eps)\n",
    "    gguf_writer.add_float32('rope_theta', config.rope_theta)\n",
    "\n",
    "\n",
    "    # write the tokenizer special token(we only need to know [PAD])\n",
    "    KEY_PAD_ID = 'tokenizer.ggml.padding_token_id'\n",
    "    gguf_writer.add_int32(KEY_PAD_ID, tokenizer.pad_token_id)\n",
    "\n",
    "    # write tensors\n",
    "    print('TENSORS')\n",
    "    hidden_size = config.hidden_size\n",
    "    for name, data in model.state_dict().items():\n",
    "        # get correct dtype\n",
    "        if 'attn_ln' in name or 'mlp' in name  or 'bias' in name or 'proj' in name or 'LayerNorm' in name:\n",
    "            dtype = torch.float32\n",
    "        else:\n",
    "            dtype = dtype0\n",
    "        shape_str = str(list(data.shape))\n",
    "        print(f'{name:64s} = {shape_str:16s} {data.dtype} → {dtype}')\n",
    "\n",
    "        # do conversion\n",
    "        data = data.to(dtype)\n",
    "\n",
    "        # add to gguf output\n",
    "        gguf_writer.add_tensor(name, data.numpy())\n",
    "\n",
    "    # execute and close writer\n",
    "    gguf_writer.write_header_to_file()\n",
    "    gguf_writer.write_kv_data_to_file()\n",
    "    gguf_writer.write_tensors_to_file()\n",
    "    gguf_writer.close()\n",
    "\n",
    "    # print success\n",
    "    print()\n",
    "    print(f'GGML model written to {output_path}')\n",
    "\n",
    "repo_id = 'Snowflake/snowflake-arctic-embed-m-v2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e875382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GteConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"GteModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"Snowflake/snowflake-arctic-embed-m-v2.0--configuration_hf_alibaba_nlp_gte.GteConfig\",\n",
      "    \"AutoModel\": \"Snowflake/snowflake-arctic-embed-m-v2.0--modeling_hf_alibaba_nlp_gte.GteModel\"\n",
      "  },\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"layer_norm_type\": \"layer_norm\",\n",
      "  \"logn_attention_clip1\": false,\n",
      "  \"logn_attention_scale\": false,\n",
      "  \"matryoshka_dimensions\": [\n",
      "    256\n",
      "  ],\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"gte\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pack_qkv\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rope\",\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 160000,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"unpad_inputs\": \"true\",\n",
      "  \"use_memory_efficient_attention\": \"true\",\n",
      "  \"vocab_size\": 250048\n",
      "}\n",
      "\n",
      "PARAMS\n",
      "vocab_size               = 250048\n",
      "hidden_size              = 768\n",
      "num_hidden_layers        = 12\n",
      "num_attention_heads      = 12\n",
      "intermediate_size        = 3072\n",
      "type_vocab_size          = 1\n",
      "pad_token_id             = 1\n",
      "\n",
      "VOCAB\n",
      "vocab_size               = 250002\n",
      "pad_token_id             = 1\n",
      "unk_token_id             = 3\n",
      "cls_token_id             = 0\n",
      "sep_token_id             = 2\n",
      "TENSORS\n",
      "embeddings.word_embeddings.weight                                = [250048, 768]    torch.float32 → torch.float16\n",
      "embeddings.token_type_embeddings.weight                          = [1, 768]         torch.float32 → torch.float16\n",
      "embeddings.LayerNorm.weight                                      = [768]            torch.float32 → torch.float32\n",
      "embeddings.LayerNorm.bias                                        = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.0.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.0.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.0.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.0.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.0.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.0.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.1.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.1.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.1.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.1.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.1.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.1.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.2.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.2.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.2.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.2.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.2.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.2.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.3.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.3.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.3.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.3.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.3.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.3.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.4.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.4.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.4.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.4.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.4.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.4.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.5.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.5.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.5.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.5.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.5.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.5.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.6.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.6.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.6.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.6.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.6.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.6.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.7.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.7.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.7.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.7.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.7.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.7.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.8.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.8.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.8.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.8.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.8.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.8.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.attention.qkv_proj.weight                        = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.9.attention.qkv_proj.bias                          = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.9.attention.o_proj.weight                          = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.9.attention.o_proj.bias                            = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.mlp.up_gate_proj.weight                          = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.9.mlp.down_proj.weight                             = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.9.mlp.down_proj.bias                               = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.attn_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.attn_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.mlp_ln.weight                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.9.mlp_ln.bias                                      = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.attention.qkv_proj.weight                       = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.10.attention.qkv_proj.bias                         = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.10.attention.o_proj.weight                         = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.10.attention.o_proj.bias                           = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.mlp.up_gate_proj.weight                         = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.10.mlp.down_proj.weight                            = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.10.mlp.down_proj.bias                              = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.attn_ln.weight                                  = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.attn_ln.bias                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.mlp_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.10.mlp_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.attention.qkv_proj.weight                       = [2304, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.11.attention.qkv_proj.bias                         = [2304]           torch.float32 → torch.float32\n",
      "encoder.layer.11.attention.o_proj.weight                         = [768, 768]       torch.float32 → torch.float32\n",
      "encoder.layer.11.attention.o_proj.bias                           = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.mlp.up_gate_proj.weight                         = [6144, 768]      torch.float32 → torch.float32\n",
      "encoder.layer.11.mlp.down_proj.weight                            = [768, 3072]      torch.float32 → torch.float32\n",
      "encoder.layer.11.mlp.down_proj.bias                              = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.attn_ln.weight                                  = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.attn_ln.bias                                    = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.mlp_ln.weight                                   = [768]            torch.float32 → torch.float32\n",
      "encoder.layer.11.mlp_ln.bias                                     = [768]            torch.float32 → torch.float32\n",
      "\n",
      "GGML model written to ../models/snowflake-arctic-embed-m-v2.0.fp16.gguf\n"
     ]
    }
   ],
   "source": [
    "convert_hf(repo_id, \"../models/snowflake-arctic-embed-m-v2.0.fp16.gguf\", float_type=\"f16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1abaa8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,    62, 57571,  7515,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "tensor([[-4.3709e-02,  2.4354e-02, -7.3933e-02,  4.0558e-02, -1.8041e-02,\n",
      "         -3.3501e-02, -2.5619e-02,  3.6312e-02,  3.0880e-02,  5.9489e-02,\n",
      "         -1.7734e-02, -3.0890e-02, -1.1997e-01,  1.1110e-02,  7.1297e-03,\n",
      "          4.8610e-02,  4.0952e-02, -2.6016e-02,  1.5491e-02, -4.4817e-02,\n",
      "          9.6097e-02, -3.0035e-02, -5.5181e-03, -3.1832e-03, -4.5806e-02,\n",
      "          6.3278e-02, -4.3031e-02, -2.5369e-02,  3.2892e-02, -2.6149e-02,\n",
      "         -7.2569e-02, -4.7534e-02, -2.7244e-02, -2.5381e-02,  3.2374e-02,\n",
      "          9.1461e-03, -3.5053e-02, -6.0449e-02,  2.9943e-02,  7.7972e-03,\n",
      "         -4.3188e-02, -4.4234e-02, -2.6241e-02,  8.7767e-03,  6.1768e-03,\n",
      "          2.6258e-02, -4.2306e-02,  4.2851e-02, -1.7615e-02, -1.8596e-02,\n",
      "         -4.2864e-02,  5.6358e-02,  6.2782e-03,  2.8444e-02, -6.2633e-03,\n",
      "          1.2718e-02, -2.2053e-02,  6.7535e-03,  5.7951e-02, -1.0903e-02,\n",
      "         -5.1803e-02, -9.2974e-03, -4.6139e-02, -3.6771e-04,  2.3313e-02,\n",
      "         -1.0941e-02, -2.0131e-02, -2.3701e-02,  7.6218e-02,  4.1238e-02,\n",
      "         -4.9146e-02,  1.8957e-02, -7.8555e-02,  5.7967e-03,  7.1961e-02,\n",
      "         -5.6054e-03, -1.4065e-02, -9.7159e-03,  1.7829e-02, -2.2851e-02,\n",
      "         -3.1694e-02, -2.5633e-02,  2.6818e-02,  6.0941e-03, -5.2842e-02,\n",
      "          1.5207e-02, -6.5011e-02, -5.6634e-02,  2.1475e-02,  4.7203e-02,\n",
      "         -3.7385e-03,  2.7331e-02, -1.4502e-02, -2.1895e-02,  2.9754e-02,\n",
      "         -5.6466e-02, -6.3348e-02, -3.2770e-02,  1.0422e-02, -8.3722e-03,\n",
      "          9.8288e-03, -4.9162e-03,  2.5674e-02,  1.5564e-02,  1.5190e-02,\n",
      "         -4.9634e-02,  2.0295e-02,  8.4551e-04,  1.4434e-02,  1.3745e-02,\n",
      "         -1.4814e-03,  3.8642e-03, -2.4733e-02,  9.7884e-03, -6.1734e-02,\n",
      "          3.9419e-02,  6.6917e-02, -8.0180e-02, -8.1378e-04,  6.1620e-02,\n",
      "         -2.2171e-02, -3.0402e-02,  3.8543e-02,  1.8510e-02, -1.3812e-01,\n",
      "         -1.5592e-02, -5.4922e-02, -1.0959e-02, -2.6262e-02, -4.9905e-02,\n",
      "          9.2859e-02,  5.4769e-03, -9.3350e-03, -5.8118e-02,  4.0993e-02,\n",
      "         -4.3305e-02, -2.8104e-02,  7.3960e-02, -1.4684e-02, -1.9249e-02,\n",
      "         -1.0990e-01,  1.1222e-02, -8.9267e-02,  8.7085e-03, -2.3095e-02,\n",
      "         -2.9636e-02, -3.9062e-02, -3.8613e-02,  2.0049e-02,  3.2889e-02,\n",
      "          2.1912e-02,  2.7227e-02,  4.4010e-02, -2.3425e-02, -2.0984e-02,\n",
      "         -2.5540e-02, -7.0467e-03,  7.6676e-02,  2.0330e-03,  3.8979e-02,\n",
      "          4.0433e-02, -2.4686e-03,  8.8800e-03, -1.1376e-02,  5.4688e-03,\n",
      "         -4.8805e-02, -6.2677e-02,  2.9081e-02,  8.7971e-02,  7.3697e-02,\n",
      "         -1.2531e-01,  1.3940e-02,  4.4098e-02,  7.4263e-02, -2.2664e-02,\n",
      "          1.5880e-02, -2.5171e-03,  1.4994e-02, -9.6323e-02, -3.9173e-02,\n",
      "         -3.2941e-02,  1.1543e-02,  1.9194e-02,  1.8126e-02,  2.6407e-03,\n",
      "          1.2016e-02, -5.6232e-02,  1.4319e-02, -2.2337e-03,  2.1967e-02,\n",
      "         -4.2771e-02, -2.2582e-03, -1.0171e-01,  1.2317e-02,  2.3911e-02,\n",
      "         -5.7513e-02,  2.3341e-02,  6.1749e-02,  1.8145e-03,  5.3052e-02,\n",
      "          2.2085e-02, -5.4649e-02, -2.3294e-02,  4.9283e-03, -3.8407e-02,\n",
      "         -8.7458e-03, -3.8571e-02, -5.4849e-02, -8.0424e-03, -1.4244e-01,\n",
      "          6.6613e-03,  5.5991e-02, -2.1268e-02, -4.9623e-05,  1.2398e-02,\n",
      "          2.5205e-02, -7.1069e-02,  6.4884e-03,  3.8440e-02,  3.2438e-02,\n",
      "         -8.0313e-03, -1.8304e-02, -2.2190e-02,  3.4784e-02,  5.6212e-02,\n",
      "          7.1192e-03, -1.2394e-02, -9.9634e-03,  6.1445e-02, -4.2664e-02,\n",
      "          4.4732e-02, -1.2972e-02,  1.7711e-02,  2.9374e-02,  1.0542e-02,\n",
      "          4.1157e-02, -2.1551e-02, -6.4444e-02, -1.4129e-02,  6.2831e-02,\n",
      "          2.8159e-02, -2.8104e-02,  3.3081e-02,  4.7588e-02,  5.7722e-02,\n",
      "          5.0468e-02, -3.4830e-02,  8.7452e-03,  4.9739e-02, -3.0663e-02,\n",
      "         -6.1120e-03,  1.8585e-02, -3.3952e-02, -4.5420e-02, -1.8091e-03,\n",
      "         -7.9115e-02, -1.5692e-02, -2.7741e-02, -2.4174e-02,  6.4536e-02,\n",
      "         -5.8066e-02,  7.6349e-03,  2.3248e-02, -5.6392e-03, -3.6199e-02,\n",
      "          1.6553e-02,  7.7732e-02,  6.1960e-02, -1.9043e-02,  3.8215e-02,\n",
      "         -1.9732e-02,  6.4596e-02,  2.2990e-03,  5.4841e-02,  2.4349e-02,\n",
      "          4.3579e-03, -2.8982e-02, -3.1261e-02, -3.7946e-02,  1.1023e-02,\n",
      "          2.4232e-02, -1.6132e-03, -3.0626e-02,  5.5653e-02,  1.2437e-02,\n",
      "          1.9428e-02, -1.5314e-02, -4.4112e-02, -1.2528e-02,  1.9265e-02,\n",
      "          2.4641e-03,  2.8663e-02, -3.3358e-02, -2.2768e-03,  4.9192e-02,\n",
      "          2.8973e-02, -2.6274e-02,  1.5760e-02,  2.1587e-02,  5.5271e-02,\n",
      "         -2.4135e-02, -7.0083e-02,  1.8689e-02, -2.7401e-02,  2.5617e-03,\n",
      "          5.9248e-02, -2.6844e-02, -5.4684e-02, -4.1114e-03, -1.4113e-02,\n",
      "         -3.7714e-02, -1.6867e-02,  3.9960e-02, -2.8817e-02,  3.1125e-02,\n",
      "         -5.4598e-02,  3.6080e-02,  8.2175e-02, -2.6202e-02, -2.3643e-02,\n",
      "         -3.0013e-02, -4.6311e-03, -5.0171e-02,  4.4553e-02, -4.8164e-02,\n",
      "         -2.0122e-02,  2.0443e-02,  1.9958e-02, -2.7353e-02, -2.6431e-04,\n",
      "          2.5598e-02,  2.3234e-02,  5.6506e-02, -2.5805e-02,  5.2334e-02,\n",
      "         -5.3265e-02, -1.9524e-02, -5.2674e-03,  2.4132e-02,  1.6430e-02,\n",
      "          9.8465e-03,  2.5713e-02, -4.7024e-02, -7.6167e-03,  6.8597e-03,\n",
      "          5.1799e-03,  1.6572e-02, -4.0054e-02,  5.0275e-03, -3.5703e-02,\n",
      "         -2.7668e-02,  1.9498e-02, -2.6122e-02,  3.7548e-02,  3.9646e-02,\n",
      "         -1.9784e-02,  2.1815e-02, -1.3288e-02, -5.6464e-02, -1.0856e-02,\n",
      "          1.3682e-02,  1.1787e-02,  4.1998e-03,  4.4867e-02,  4.1218e-02,\n",
      "          1.4197e-02,  1.6614e-02,  3.2872e-02, -1.5150e-02, -1.1059e-02,\n",
      "         -2.8062e-02, -9.2258e-03, -1.0117e-02,  6.6686e-04, -3.9750e-02,\n",
      "          2.0637e-02, -2.0575e-02, -2.0526e-02, -2.5482e-02,  8.5390e-03,\n",
      "          4.5421e-02,  1.8908e-03,  4.5172e-02, -2.1998e-02,  6.5628e-03,\n",
      "         -4.1630e-02,  4.3908e-03, -5.2507e-03, -7.0695e-03, -3.1787e-03,\n",
      "          5.9141e-02,  4.5386e-03, -5.5311e-02,  2.5975e-02,  7.7998e-02,\n",
      "          1.2031e-02,  2.2811e-02, -2.1739e-02, -2.8171e-02, -1.7217e-02,\n",
      "          2.6955e-02,  3.6090e-02,  2.2812e-03,  1.6920e-03, -1.8623e-02,\n",
      "          1.9177e-02, -3.9719e-03,  3.8186e-03, -7.8912e-03, -4.5096e-02,\n",
      "          2.3995e-02, -2.7216e-02,  1.7932e-02, -2.9385e-02, -4.4645e-03,\n",
      "          3.5962e-02, -6.1011e-03, -8.3681e-03,  1.0250e-01, -2.9091e-02,\n",
      "          1.4044e-02,  2.6632e-02,  1.8797e-02, -5.8930e-03, -1.1441e-02,\n",
      "         -1.1102e-02, -2.1991e-03, -1.6512e-03, -3.7506e-02,  1.3284e-02,\n",
      "         -4.9140e-02, -7.9658e-03, -7.0764e-02, -5.6468e-02,  1.7040e-03,\n",
      "         -1.5158e-02,  2.2432e-02,  2.4688e-02, -5.1761e-02, -3.7164e-02,\n",
      "         -2.2475e-02,  2.4068e-03, -1.2942e-04, -2.5549e-02,  2.0744e-02,\n",
      "         -1.6604e-03, -2.7487e-02,  5.0488e-03,  1.6376e-02,  4.1284e-02,\n",
      "         -5.7952e-02,  1.6045e-02,  1.5488e-02,  2.7379e-02, -4.5100e-02,\n",
      "          2.6899e-02, -4.9380e-03,  1.1856e-02,  1.3348e-02,  3.0322e-02,\n",
      "          4.9477e-03,  1.3892e-02,  9.2328e-03,  2.6769e-02,  7.0721e-03,\n",
      "          4.3787e-02, -7.7569e-03,  3.0380e-03,  1.4815e-02,  1.0208e-02,\n",
      "         -4.1253e-02, -2.9838e-02, -4.0299e-02, -1.5690e-02, -2.1099e-02,\n",
      "          3.5494e-02,  2.0768e-02,  4.4715e-03, -3.2405e-02,  1.4259e-02,\n",
      "          2.9723e-02,  5.7450e-02,  3.0765e-02, -6.5931e-03, -2.2922e-02,\n",
      "         -7.2646e-03, -1.3032e-02, -2.6383e-02,  1.5104e-02,  6.4291e-02,\n",
      "          3.2863e-02,  2.9176e-02,  2.5088e-02, -3.8241e-02,  3.5509e-02,\n",
      "         -8.3598e-03, -6.0006e-03,  4.4989e-02,  2.8682e-02, -1.4524e-02,\n",
      "          2.2881e-03,  1.7375e-02, -5.9783e-03,  1.2232e-02,  4.2722e-03,\n",
      "         -3.1433e-02,  9.5255e-03,  2.7032e-02,  5.1986e-03,  1.3480e-02,\n",
      "         -1.3862e-02, -3.5081e-03,  4.8420e-02, -3.6110e-02, -4.1733e-02,\n",
      "         -3.1844e-02, -3.5582e-02,  2.5427e-02, -1.0830e-02, -2.4400e-02,\n",
      "         -2.5266e-02,  3.2219e-02, -1.5527e-02,  6.9159e-02,  4.4035e-02,\n",
      "         -4.9430e-02,  2.8447e-02,  3.1666e-02,  2.1025e-02, -1.5050e-02,\n",
      "          5.1609e-02,  2.7888e-02,  2.0587e-02,  9.1835e-03,  3.4263e-02,\n",
      "          5.1984e-02,  1.9744e-03,  3.7669e-02, -4.2232e-02, -3.1896e-02,\n",
      "         -3.0987e-02, -1.9962e-02, -3.0103e-02, -3.6519e-02,  2.5284e-03,\n",
      "          5.4776e-02,  9.5935e-02, -5.5329e-02,  6.6713e-03, -3.5721e-02,\n",
      "         -1.9074e-03, -3.5610e-03, -3.4311e-02,  4.7603e-02, -3.2534e-02,\n",
      "         -1.2238e-03,  8.3939e-03,  2.5697e-02,  2.0888e-02,  3.4504e-02,\n",
      "         -1.0884e-02,  1.6126e-02,  1.7070e-02, -3.5871e-02, -5.2060e-02,\n",
      "          2.2207e-02,  1.5113e-02, -1.3987e-02,  4.5454e-02,  3.6536e-02,\n",
      "          4.9329e-02, -3.4348e-02,  3.6597e-02, -1.9076e-03,  4.2794e-02,\n",
      "         -1.9298e-02, -4.3476e-02, -2.8609e-02, -1.1531e-02,  3.1974e-02,\n",
      "         -8.3857e-02, -3.6010e-02,  2.6672e-02,  1.2935e-02,  1.8342e-02,\n",
      "          4.8630e-02, -5.0996e-02,  1.6559e-03,  8.5641e-03, -1.1434e-02,\n",
      "         -1.0206e-02,  4.1649e-02,  2.8993e-02,  2.5418e-04,  3.6880e-02,\n",
      "         -1.2440e-03,  2.1138e-02, -3.7467e-02,  1.7476e-02, -1.8671e-02,\n",
      "         -2.8636e-03, -3.1750e-02,  6.0192e-04, -1.4631e-02, -5.2011e-03,\n",
      "         -1.4992e-02, -1.0571e-02, -1.3919e-02, -3.2038e-02,  9.0100e-03,\n",
      "         -3.1526e-02, -2.7086e-02,  2.2216e-02, -7.1340e-03,  5.0197e-02,\n",
      "         -3.9142e-02, -2.3381e-02,  1.3551e-02,  9.0363e-03,  9.8070e-03,\n",
      "         -9.7338e-03, -4.4568e-02,  4.0642e-02,  2.1222e-02,  8.3712e-03,\n",
      "         -5.6994e-02,  4.7677e-02, -1.0942e-02, -5.2998e-03,  5.1666e-03,\n",
      "         -4.2919e-03,  3.7599e-02, -6.7047e-03, -1.8775e-02,  2.7290e-02,\n",
      "          2.4769e-02, -1.3926e-03, -1.3808e-02,  6.1575e-03,  1.0437e-02,\n",
      "          3.4778e-02,  2.9382e-02,  1.9291e-02, -3.5725e-02, -5.6144e-02,\n",
      "          4.8002e-03,  3.2858e-02, -3.7019e-02, -4.8763e-02,  6.0960e-02,\n",
      "          5.8650e-02,  5.7500e-02,  3.1292e-03,  1.8795e-02, -6.3543e-04,\n",
      "         -2.7653e-02, -4.3528e-02,  5.2516e-04,  3.8177e-03,  9.7602e-02,\n",
      "          3.8378e-03,  5.9880e-03,  1.9063e-03, -3.1186e-02, -1.8153e-02,\n",
      "          3.2323e-02,  2.4176e-03,  1.5184e-02,  3.4362e-02,  3.6070e-02,\n",
      "         -9.1828e-03,  3.0648e-02,  4.5275e-02,  3.0276e-03, -6.5354e-02,\n",
      "          4.8520e-03, -3.0908e-02, -3.1046e-02,  2.7413e-02, -4.6389e-02,\n",
      "          2.7920e-02,  8.6109e-03,  1.3080e-02, -2.0981e-02, -2.6920e-02,\n",
      "         -8.3834e-02,  5.1190e-02, -1.1953e-02,  8.9224e-03,  3.1926e-02,\n",
      "         -3.4174e-03,  2.4584e-02,  5.0118e-02, -6.8893e-02,  7.2597e-02,\n",
      "         -4.6843e-02, -2.4164e-02,  1.5117e-04,  1.9296e-02, -5.0629e-02,\n",
      "         -1.9190e-02, -1.6251e-02,  1.7945e-02,  4.7776e-03, -3.5228e-02,\n",
      "          1.5620e-03,  1.8979e-02,  1.8818e-02, -1.7690e-02, -7.9274e-03,\n",
      "          2.1792e-02,  2.0949e-02, -2.5339e-02,  1.7714e-02,  1.8841e-02,\n",
      "          2.4285e-02,  3.5631e-02, -4.9584e-02, -2.2039e-02,  2.4366e-01,\n",
      "          1.0917e-02,  2.2554e-02,  2.1122e-02,  1.3430e-02,  1.2412e-02,\n",
      "          2.6020e-02,  3.1526e-02,  5.3284e-03, -3.0970e-02, -2.9181e-02,\n",
      "          1.2820e-02,  2.0430e-02,  2.4460e-02,  2.8529e-02,  2.0746e-04,\n",
      "         -4.1430e-02,  1.4057e-02,  4.3434e-02, -2.3467e-02, -2.3172e-02,\n",
      "          2.6296e-02,  1.2558e-02, -2.9046e-02, -2.4207e-03,  2.2683e-02,\n",
      "          5.6780e-03,  6.9097e-02, -1.9841e-02,  2.9776e-02, -1.7595e-04,\n",
      "         -8.2608e-03,  5.2912e-03,  2.7041e-03,  9.8421e-03,  1.1802e-02,\n",
      "         -4.4618e-02,  3.0590e-02, -3.9704e-02,  3.1665e-02, -2.2146e-02,\n",
      "          3.6653e-02, -4.6244e-03,  1.8649e-02,  1.3707e-02,  7.6407e-03,\n",
      "         -3.7097e-02, -1.5276e-02, -6.2169e-02]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = 'Snowflake/snowflake-arctic-embed-m-v2.0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, add_pooling_layer=False, trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "documents = ['A blue cat']\n",
    "document_tokens =  tokenizer(documents, padding=True, truncation=True, return_tensors='pt', max_length=8192)\n",
    "\n",
    "print(document_tokens)\n",
    "\n",
    "# Move inputs to same device as model\n",
    "document_tokens = {k: v.to(device) for k, v in document_tokens.items()}\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    document_embeddings = model(**document_tokens)[0][:, 0]\n",
    "\n",
    "\n",
    "# normalize embeddings\n",
    "document_embeddings = torch.nn.functional.normalize(document_embeddings, p=2, dim=1)\n",
    "\n",
    "print(document_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86bb4fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.37089577e-02  2.43543386e-02 -7.39330947e-02  4.05579172e-02\n",
      "  -1.80412754e-02 -3.35012674e-02 -2.56192908e-02  3.63120325e-02\n",
      "   3.08795143e-02  5.94893135e-02 -1.77344810e-02 -3.08901053e-02\n",
      "  -1.19969159e-01  1.11098858e-02  7.12967850e-03  4.86099496e-02\n",
      "   4.09523770e-02 -2.60164179e-02  1.54911261e-02 -4.48167212e-02\n",
      "   9.60970595e-02 -3.00353728e-02 -5.51805412e-03 -3.18316813e-03\n",
      "  -4.58059460e-02  6.32777736e-02 -4.30310741e-02 -2.53691040e-02\n",
      "   3.28916237e-02 -2.61493474e-02 -7.25693554e-02 -4.75339927e-02\n",
      "  -2.72440091e-02 -2.53808890e-02  3.23742777e-02  9.14607570e-03\n",
      "  -3.50533575e-02 -6.04486093e-02  2.99426056e-02  7.79717788e-03\n",
      "  -4.31877784e-02 -4.42339294e-02 -2.62413621e-02  8.77672341e-03\n",
      "   6.17679255e-03  2.62576751e-02 -4.23056409e-02  4.28507663e-02\n",
      "  -1.76152233e-02 -1.85956955e-02 -4.28638048e-02  5.63584790e-02\n",
      "   6.27824292e-03  2.84444727e-02 -6.26334362e-03  1.27175646e-02\n",
      "  -2.20532417e-02  6.75354898e-03  5.79505190e-02 -1.09032188e-02\n",
      "  -5.18030412e-02 -9.29736346e-03 -4.61390167e-02 -3.67714354e-04\n",
      "   2.33127326e-02 -1.09411152e-02 -2.01308560e-02 -2.37006117e-02\n",
      "   7.62179121e-02  4.12379503e-02 -4.91462126e-02  1.89573988e-02\n",
      "  -7.85548910e-02  5.79665741e-03  7.19606131e-02 -5.60543966e-03\n",
      "  -1.40646705e-02 -9.71593056e-03  1.78287048e-02 -2.28510723e-02\n",
      "  -3.16943154e-02 -2.56334972e-02  2.68179402e-02  6.09407295e-03\n",
      "  -5.28416671e-02  1.52073056e-02 -6.50105253e-02 -5.66342957e-02\n",
      "   2.14745700e-02  4.72031720e-02 -3.73854488e-03  2.73307394e-02\n",
      "  -1.45016918e-02 -2.18949579e-02  2.97539495e-02 -5.64660989e-02\n",
      "  -6.33481592e-02 -3.27704810e-02  1.04217092e-02 -8.37222859e-03\n",
      "   9.82876960e-03 -4.91619948e-03  2.56744511e-02  1.55637097e-02\n",
      "   1.51897967e-02 -4.96342853e-02  2.02954970e-02  8.45508999e-04\n",
      "   1.44340647e-02  1.37449484e-02 -1.48137775e-03  3.86421522e-03\n",
      "  -2.47333087e-02  9.78842750e-03 -6.17344454e-02  3.94187793e-02\n",
      "   6.69166148e-02 -8.01803321e-02 -8.13780935e-04  6.16196059e-02\n",
      "  -2.21710894e-02 -3.04024648e-02  3.85434628e-02  1.85097642e-02\n",
      "  -1.38122827e-01 -1.55923450e-02 -5.49220219e-02 -1.09589268e-02\n",
      "  -2.62621325e-02 -4.99049835e-02  9.28589031e-02  5.47692319e-03\n",
      "  -9.33496654e-03 -5.81180342e-02  4.09926213e-02 -4.33053225e-02\n",
      "  -2.81043947e-02  7.39596486e-02 -1.46839013e-02 -1.92487594e-02\n",
      "  -1.09896548e-01  1.12221912e-02 -8.92670453e-02  8.70849751e-03\n",
      "  -2.30951943e-02 -2.96360105e-02 -3.90621163e-02 -3.86133790e-02\n",
      "   2.00485103e-02  3.28890160e-02  2.19120104e-02  2.72270683e-02\n",
      "   4.40102741e-02 -2.34248657e-02 -2.09843758e-02 -2.55399253e-02\n",
      "  -7.04669766e-03  7.66758174e-02  2.03295960e-03  3.89790647e-02\n",
      "   4.04329523e-02 -2.46860320e-03  8.88001733e-03 -1.13761108e-02\n",
      "   5.46875736e-03 -4.88045402e-02 -6.26767874e-02  2.90814023e-02\n",
      "   8.79707262e-02  7.36969337e-02 -1.25312537e-01  1.39400531e-02\n",
      "   4.40984778e-02  7.42626712e-02 -2.26638298e-02  1.58800650e-02\n",
      "  -2.51705130e-03  1.49942655e-02 -9.63229313e-02 -3.91731299e-02\n",
      "  -3.29406150e-02  1.15434611e-02  1.91939846e-02  1.81257557e-02\n",
      "   2.64073582e-03  1.20162154e-02 -5.62324934e-02  1.43191759e-02\n",
      "  -2.23369990e-03  2.19669677e-02 -4.27705720e-02 -2.25819601e-03\n",
      "  -1.01711228e-01  1.23173427e-02  2.39105709e-02 -5.75126708e-02\n",
      "   2.33406592e-02  6.17492385e-02  1.81447342e-03  5.30523472e-02\n",
      "   2.20848769e-02 -5.46491556e-02 -2.32941583e-02  4.92834626e-03\n",
      "  -3.84070985e-02 -8.74579512e-03 -3.85705531e-02 -5.48488908e-02\n",
      "  -8.04244727e-03 -1.42435551e-01  6.66132802e-03  5.59914522e-02\n",
      "  -2.12681312e-02 -4.96230023e-05  1.23979179e-02  2.52046715e-02\n",
      "  -7.10686371e-02  6.48843916e-03  3.84396277e-02  3.24382745e-02\n",
      "  -8.03127140e-03 -1.83041934e-02 -2.21897271e-02  3.47840749e-02\n",
      "   5.62123433e-02  7.11921323e-03 -1.23938294e-02 -9.96342860e-03\n",
      "   6.14452139e-02 -4.26642597e-02  4.47322428e-02 -1.29715065e-02\n",
      "   1.77109502e-02  2.93742791e-02  1.05416495e-02  4.11570072e-02\n",
      "  -2.15507206e-02 -6.44436851e-02 -1.41289188e-02  6.28313869e-02\n",
      "   2.81593855e-02 -2.81038247e-02  3.30807343e-02  4.75879461e-02\n",
      "   5.77217489e-02  5.04676104e-02 -3.48302275e-02  8.74524843e-03\n",
      "   4.97391932e-02 -3.06628793e-02 -6.11203536e-03  1.85852684e-02\n",
      "  -3.39523517e-02 -4.54198234e-02 -1.80906733e-03 -7.91146606e-02\n",
      "  -1.56917870e-02 -2.77406145e-02 -2.41739322e-02  6.45357519e-02\n",
      "  -5.80656752e-02  7.63488375e-03  2.32484471e-02 -5.63915307e-03\n",
      "  -3.61987017e-02  1.65533274e-02  7.77322352e-02  6.19597472e-02\n",
      "  -1.90426912e-02  3.82148810e-02 -1.97315253e-02  6.45957738e-02\n",
      "   2.29900610e-03  5.48410788e-02  2.43487656e-02  4.35794191e-03\n",
      "  -2.89818030e-02 -3.12605612e-02 -3.79461236e-02  1.10234087e-02\n",
      "   2.42316294e-02 -1.61319890e-03 -3.06256209e-02  5.56532778e-02\n",
      "   1.24370819e-02  1.94283631e-02 -1.53144458e-02 -4.41118516e-02\n",
      "  -1.25275198e-02  1.92653369e-02  2.46413145e-03  2.86633931e-02\n",
      "  -3.33584175e-02 -2.27683666e-03  4.91922684e-02  2.89725848e-02\n",
      "  -2.62742862e-02  1.57603361e-02  2.15868391e-02  5.52714169e-02\n",
      "  -2.41345484e-02 -7.00831190e-02  1.86888222e-02 -2.74006464e-02\n",
      "   2.56166654e-03  5.92482835e-02 -2.68440023e-02 -5.46838380e-02\n",
      "  -4.11137566e-03 -1.41125470e-02 -3.77140120e-02 -1.68668628e-02\n",
      "   3.99599746e-02 -2.88170725e-02  3.11254375e-02 -5.45979068e-02\n",
      "   3.60802338e-02  8.21748227e-02 -2.62021255e-02 -2.36431696e-02\n",
      "  -3.00131403e-02 -4.63106483e-03 -5.01714498e-02  4.45532613e-02\n",
      "  -4.81644273e-02 -2.01218463e-02  2.04429906e-02  1.99577007e-02\n",
      "  -2.73530614e-02 -2.64312577e-04  2.55975798e-02  2.32337341e-02\n",
      "   5.65060489e-02 -2.58046649e-02  5.23344651e-02 -5.32649159e-02\n",
      "  -1.95237175e-02 -5.26744965e-03  2.41315253e-02  1.64295156e-02\n",
      "   9.84648429e-03  2.57134847e-02 -4.70242128e-02 -7.61668058e-03\n",
      "   6.85974723e-03  5.17992768e-03  1.65720508e-02 -4.00537290e-02\n",
      "   5.02749300e-03 -3.57029326e-02 -2.76677832e-02  1.94983855e-02\n",
      "  -2.61220317e-02  3.75476405e-02  3.96461822e-02 -1.97836850e-02\n",
      "   2.18152329e-02 -1.32879624e-02 -5.64643070e-02 -1.08562438e-02\n",
      "   1.36817498e-02  1.17865950e-02  4.19975817e-03  4.48667519e-02\n",
      "   4.12178785e-02  1.41967777e-02  1.66139286e-02  3.28724422e-02\n",
      "  -1.51498793e-02 -1.10591985e-02 -2.80618910e-02 -9.22583882e-03\n",
      "  -1.01165017e-02  6.66862528e-04 -3.97500545e-02  2.06366796e-02\n",
      "  -2.05754265e-02 -2.05256958e-02 -2.54819952e-02  8.53900146e-03\n",
      "   4.54206727e-02  1.89075491e-03  4.51715253e-02 -2.19980367e-02\n",
      "   6.56278664e-03 -4.16298620e-02  4.39082040e-03 -5.25073474e-03\n",
      "  -7.06949411e-03 -3.17872595e-03  5.91411553e-02  4.53862548e-03\n",
      "  -5.53107522e-02  2.59748194e-02  7.79978260e-02  1.20305493e-02\n",
      "   2.28107236e-02 -2.17387937e-02 -2.81709787e-02 -1.72170177e-02\n",
      "   2.69550681e-02  3.60901579e-02  2.28120503e-03  1.69202907e-03\n",
      "  -1.86227169e-02  1.91766862e-02 -3.97193152e-03  3.81859741e-03\n",
      "  -7.89123867e-03 -4.50956598e-02  2.39951350e-02 -2.72160266e-02\n",
      "   1.79324616e-02 -2.93852873e-02 -4.46445029e-03  3.59619521e-02\n",
      "  -6.10109139e-03 -8.36805161e-03  1.02495529e-01 -2.90907826e-02\n",
      "   1.40443016e-02  2.66324133e-02  1.87972132e-02 -5.89300925e-03\n",
      "  -1.14412354e-02 -1.11015365e-02 -2.19913153e-03 -1.65119546e-03\n",
      "  -3.75064388e-02  1.32840695e-02 -4.91403937e-02 -7.96583202e-03\n",
      "  -7.07639307e-02 -5.64682633e-02  1.70397328e-03 -1.51581103e-02\n",
      "   2.24317834e-02  2.46882848e-02 -5.17610386e-02 -3.71635668e-02\n",
      "  -2.24751849e-02  2.40679318e-03 -1.29420208e-04 -2.55485326e-02\n",
      "   2.07439009e-02 -1.66041346e-03 -2.74867751e-02  5.04876301e-03\n",
      "   1.63757354e-02  4.12838981e-02 -5.79516850e-02  1.60448346e-02\n",
      "   1.54878795e-02  2.73787118e-02 -4.51002307e-02  2.68987734e-02\n",
      "  -4.93800407e-03  1.18561238e-02  1.33484630e-02  3.03217340e-02\n",
      "   4.94774897e-03  1.38918972e-02  9.23281722e-03  2.67688911e-02\n",
      "   7.07206316e-03  4.37871628e-02 -7.75687303e-03  3.03804455e-03\n",
      "   1.48153566e-02  1.02082817e-02 -4.12531346e-02 -2.98382752e-02\n",
      "  -4.02993076e-02 -1.56901460e-02 -2.10988969e-02  3.54935043e-02\n",
      "   2.07684152e-02  4.47151531e-03 -3.24049294e-02  1.42594883e-02\n",
      "   2.97226794e-02  5.74503168e-02  3.07651795e-02 -6.59305649e-03\n",
      "  -2.29221135e-02 -7.26464577e-03 -1.30319744e-02 -2.63831150e-02\n",
      "   1.51042938e-02  6.42909184e-02  3.28626335e-02  2.91762296e-02\n",
      "   2.50878874e-02 -3.82406041e-02  3.55087519e-02 -8.35978147e-03\n",
      "  -6.00062404e-03  4.49885875e-02  2.86821797e-02 -1.45240752e-02\n",
      "   2.28807866e-03  1.73747633e-02 -5.97832538e-03  1.22324964e-02\n",
      "   4.27217502e-03 -3.14330272e-02  9.52549465e-03  2.70317867e-02\n",
      "   5.19855553e-03  1.34804491e-02 -1.38620464e-02 -3.50807491e-03\n",
      "   4.84204218e-02 -3.61095555e-02 -4.17329669e-02 -3.18438485e-02\n",
      "  -3.55824344e-02  2.54274644e-02 -1.08295856e-02 -2.43998617e-02\n",
      "  -2.52663326e-02  3.22191119e-02 -1.55268870e-02  6.91593438e-02\n",
      "   4.40347157e-02 -4.94297706e-02  2.84474473e-02  3.16659734e-02\n",
      "   2.10245699e-02 -1.50496950e-02  5.16088121e-02  2.78875716e-02\n",
      "   2.05870010e-02  9.18351393e-03  3.42628546e-02  5.19839339e-02\n",
      "   1.97438453e-03  3.76690738e-02 -4.22319286e-02 -3.18962336e-02\n",
      "  -3.09868529e-02 -1.99620035e-02 -3.01030148e-02 -3.65194902e-02\n",
      "   2.52843089e-03  5.47756925e-02  9.59346443e-02 -5.53287454e-02\n",
      "   6.67131832e-03 -3.57213542e-02 -1.90741674e-03 -3.56102875e-03\n",
      "  -3.43112424e-02  4.76033054e-02 -3.25344317e-02 -1.22375775e-03\n",
      "   8.39392655e-03  2.56974809e-02  2.08876971e-02  3.45042199e-02\n",
      "  -1.08835483e-02  1.61256287e-02  1.70704871e-02 -3.58711556e-02\n",
      "  -5.20597436e-02  2.22071111e-02  1.51134692e-02 -1.39872879e-02\n",
      "   4.54541557e-02  3.65357250e-02  4.93285507e-02 -3.43482792e-02\n",
      "   3.65970321e-02 -1.90757040e-03  4.27937545e-02 -1.92979332e-02\n",
      "  -4.34757546e-02 -2.86089387e-02 -1.15311118e-02  3.19738388e-02\n",
      "  -8.38565081e-02 -3.60099226e-02  2.66715884e-02  1.29347239e-02\n",
      "   1.83415934e-02  4.86299992e-02 -5.09962030e-02  1.65593508e-03\n",
      "   8.56407918e-03 -1.14335287e-02 -1.02055967e-02  4.16488163e-02\n",
      "   2.89932583e-02  2.54177023e-04  3.68797481e-02 -1.24402822e-03\n",
      "   2.11381763e-02 -3.74670699e-02  1.74764749e-02 -1.86710414e-02\n",
      "  -2.86359759e-03 -3.17496881e-02  6.01923501e-04 -1.46311549e-02\n",
      "  -5.20107662e-03 -1.49921058e-02 -1.05707161e-02 -1.39186438e-02\n",
      "  -3.20376903e-02  9.01003554e-03 -3.15259248e-02 -2.70858258e-02\n",
      "   2.22162176e-02 -7.13398308e-03  5.01970574e-02 -3.91424447e-02\n",
      "  -2.33813953e-02  1.35511803e-02  9.03631654e-03  9.80698876e-03\n",
      "  -9.73380171e-03 -4.45680842e-02  4.06421162e-02  2.12220792e-02\n",
      "   8.37119296e-03 -5.69944009e-02  4.76766750e-02 -1.09421536e-02\n",
      "  -5.29979868e-03  5.16656879e-03 -4.29190649e-03  3.75987180e-02\n",
      "  -6.70467597e-03 -1.87750086e-02  2.72902288e-02  2.47689746e-02\n",
      "  -1.39263493e-03 -1.38084991e-02  6.15747925e-03  1.04371617e-02\n",
      "   3.47784720e-02  2.93823797e-02  1.92914642e-02 -3.57251763e-02\n",
      "  -5.61443083e-02  4.80022747e-03  3.28584984e-02 -3.70187834e-02\n",
      "  -4.87632230e-02  6.09597415e-02  5.86502030e-02  5.74999824e-02\n",
      "   3.12919240e-03  1.87947284e-02 -6.35431963e-04 -2.76531037e-02\n",
      "  -4.35284674e-02  5.25163428e-04  3.81772500e-03  9.76023600e-02\n",
      "   3.83780268e-03  5.98803582e-03  1.90629624e-03 -3.11861020e-02\n",
      "  -1.81530360e-02  3.23234983e-02  2.41762423e-03  1.51838828e-02\n",
      "   3.43622491e-02  3.60704623e-02 -9.18279029e-03  3.06480788e-02\n",
      "   4.52749357e-02  3.02759442e-03 -6.53541163e-02  4.85199550e-03\n",
      "  -3.09080295e-02 -3.10462862e-02  2.74132155e-02 -4.63890247e-02\n",
      "   2.79203374e-02  8.61088652e-03  1.30801313e-02 -2.09807884e-02\n",
      "  -2.69197915e-02 -8.38339403e-02  5.11897579e-02 -1.19531574e-02\n",
      "   8.92235246e-03  3.19262259e-02 -3.41735827e-03  2.45841742e-02\n",
      "   5.01176380e-02 -6.88931122e-02  7.25969374e-02 -4.68429402e-02\n",
      "  -2.41640899e-02  1.51169079e-04  1.92963257e-02 -5.06290831e-02\n",
      "  -1.91896856e-02 -1.62508432e-02  1.79454349e-02  4.77759214e-03\n",
      "  -3.52277122e-02  1.56202866e-03  1.89788509e-02  1.88180506e-02\n",
      "  -1.76897999e-02 -7.92741962e-03  2.17917841e-02  2.09494177e-02\n",
      "  -2.53390837e-02  1.77139398e-02  1.88408531e-02  2.42848489e-02\n",
      "   3.56307961e-02 -4.95836362e-02 -2.20391247e-02  2.43662387e-01\n",
      "   1.09168049e-02  2.25543398e-02  2.11216658e-02  1.34297889e-02\n",
      "   1.24116885e-02  2.60200389e-02  3.15263905e-02  5.32840751e-03\n",
      "  -3.09696961e-02 -2.91809365e-02  1.28199095e-02  2.04299875e-02\n",
      "   2.44597644e-02  2.85292827e-02  2.07463323e-04 -4.14302982e-02\n",
      "   1.40573159e-02  4.34340797e-02 -2.34667510e-02 -2.31715348e-02\n",
      "   2.62956806e-02  1.25581110e-02 -2.90456917e-02 -2.42066500e-03\n",
      "   2.26829574e-02  5.67797013e-03  6.90974966e-02 -1.98405776e-02\n",
      "   2.97758374e-02 -1.75953101e-04 -8.26078095e-03  5.29116485e-03\n",
      "   2.70405272e-03  9.84208472e-03  1.18022766e-02 -4.46178615e-02\n",
      "   3.05895619e-02 -3.97037901e-02  3.16646621e-02 -2.21464317e-02\n",
      "   3.66530716e-02 -4.62435558e-03  1.86488032e-02  1.37067148e-02\n",
      "   7.64072686e-03 -3.70972939e-02 -1.52757736e-02 -6.21691793e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "model_name = 'Snowflake/snowflake-arctic-embed-m-v2.0'\n",
    "model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "document_embeddings = model.encode(documents)\n",
    "\n",
    "print(document_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
