# Alignment Comparison Tool

This tool compares embeddings generated by the C++ implementation (embeddings.cpp) with the reference transformers library implementation.

## Usage

### Basic Usage

Simply run the alignment script:

```bash
cd scripts
python alignment.py
```

### Configuration

#### 1. Models Configuration

Edit `models.txt` to specify which models to test:

```
BAAI/bge-m3
BAAI/bge-base-zh-v1.5
shibing624/text2vec-base-multilingual
sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
Snowflake/snowflake-arctic-embed-m-v2.0
```

Lines starting with `#` are treated as comments and ignored.

#### 2. Test Prompts Configuration

Edit `prompts.txt` to customize test prompts:

```
你好，今天天气怎么样？
What's the weather like today?
The quick brown fox jumps over the lazy dog.
机器学习是人工智能的一个重要分支。
```

If `prompts.txt` doesn't exist, default prompts will be used.

### Prerequisites

1. **GGUF Models**: Ensure you have converted your models to GGUF format and placed them in the `../models/` directory. The tool expects filenames in the format `{model-name}.fp16.gguf`.

2. **Python Dependencies**: Install required packages:
   ```bash
   pip install -r requirements.txt
   ```

3. **Compiled C++ Library**: Make sure `embeddings_cpp` is properly built and in your Python path.

### Output

The tool generates two output files in the `output/` directory:

1. **CSV Format**: `alignment_results_YYYYMMDD_HHMMSS.csv`
   - Machine-readable format for further analysis
   - Contains: repo_name, gguf_file_name, mse, cosine_similarity, embedding_dim, num_prompts, status

2. **Markdown Format**: `alignment_results_YYYYMMDD_HHMMSS.md`
   - Human-readable format with summary table and detailed results
   - Includes timestamp and comprehensive comparison results

### Metrics

The tool calculates two key metrics to compare the embeddings:

1. **Mean Squared Error (MSE)**: Measures the average squared difference between corresponding embeddings
   - Lower values indicate better alignment
   - Typical good values are in the range of 1e-6 to 1e-9

2. **Cosine Similarity**: Measures the cosine of the angle between embedding vectors
   - Values range from -1 to 1, where 1 indicates perfect similarity
   - Values close to 1.0 (e.g., > 0.999) indicate excellent alignment

### Model-Specific Configuration

The tool automatically detects the appropriate pooling method for each model:

- **CLS Pooling**: BAAI/bge-m3, BAAI/bge-base-zh-v1.5, Snowflake/snowflake-arctic-embed-m-v2.0
- **Mean Pooling**: shibing624/text2vec-base-multilingual, sentence-transformers models

### Expected File Structure

```
scripts/
├── alignment.py           # Main comparison script
├── models.txt            # List of models to test
├── prompts.txt           # Test prompts (optional)
├── requirements.txt      # Python dependencies
├── output/              # Generated results
└── ../models/           # GGUF model files
    ├── bge-m3.fp16.gguf
    ├── bge-base-zh-v1.5.fp16.gguf
    └── ...
```

### Troubleshooting

1. **Missing GGUF files**: Ensure model files are properly converted and named
2. **Import errors**: Check that embeddings_cpp is properly compiled and installed
3. **Memory issues**: Reduce the number of test prompts or test fewer models at once
4. **Network issues**: Some models require internet access for initial download

### Sample Output

```
Testing model: BAAI/bge-m3
========================
Loading C++ model: bge-m3.fp16.gguf
Loading transformers model: BAAI/bge-m3
MSE: 1.23e-09
Cosine Similarity: 0.999999
Embedding dimension: 1024

Results saved:
CSV: output/alignment_results_20250630_143022.csv
Markdown: output/alignment_results_20250630_143022.md

Summary: 3/5 models tested successfully
```
