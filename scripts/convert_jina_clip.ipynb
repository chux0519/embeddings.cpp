{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuxd\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from gguf import GGUFWriter, GGMLQuantizationType\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:2081\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:2081\"\n",
    "\n",
    "def convert_hf(repo_id, output_path, float_type='f16'):\n",
    "    # convert to ggml quantization type\n",
    "    if float_type not in ['f16', 'f32']:\n",
    "        print(f'Float type must be f16 or f32, got: {float_type}')\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        qtype = GGMLQuantizationType[float_type.upper()]\n",
    "        dtype0 = {'f16': torch.float16, 'f32': torch.float32}[float_type]\n",
    "\n",
    "    # load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "    tokenizer_json = os.path.join(os.path.dirname(output_path), os.path.basename(repo_id) + \".tokenizer.json\")\n",
    "    # tokenizer.save_pretrained(tokenizer_json)\n",
    "    tokenizer._tokenizer.save(tokenizer_json, False)\n",
    "\n",
    "    model = AutoModel.from_pretrained(repo_id, trust_remote_code=True)\n",
    "\n",
    "    if model.text_model:\n",
    "        model = model.text_model\n",
    "\n",
    "    config = model.config\n",
    "    print(config)\n",
    "    # check: https://huggingface.co/jinaai/jina-bert-flash-implementation/blob/main/configuration_bert.py#L62\n",
    "        # self,\n",
    "        # vocab_size=30522,\n",
    "        # hidden_size=768,\n",
    "        # num_hidden_layers=12,\n",
    "        # num_attention_heads=12,\n",
    "        # intermediate_size=3072,\n",
    "        # hidden_act=\"gelu\",\n",
    "        # hidden_dropout_prob=0.1,\n",
    "        # attention_probs_dropout_prob=0.1,\n",
    "        # type_vocab_size=2,\n",
    "        # initializer_range=0.02,\n",
    "        # layer_norm_eps=1e-12,\n",
    "        # pad_token_id=0,\n",
    "        # window_size=(-1, -1),\n",
    "        # dense_seq_output=False,\n",
    "        # mlp_type='mlp',\n",
    "        # mlp_checkpoint_lvl=0,\n",
    "        # last_layer_subset=False,\n",
    "        # fused_dropout_add_ln=False,\n",
    "        # fused_bias_fc=False,\n",
    "        # pad_vocab_size_multiple=1,\n",
    "        # use_flash_attn=True,\n",
    "        # use_qk_norm=True,\n",
    "        # emb_pooler=None,\n",
    "        # classifier_dropout=None,\n",
    "        # num_loras=5,\n",
    "        # **kwargs,\n",
    "    # print model\n",
    "    param_keys = [\n",
    "        'vocab_size', 'hidden_size', 'num_hidden_layers',\n",
    "        'num_attention_heads', 'intermediate_size', 'type_vocab_size', 'pad_token_id'\n",
    "    ]\n",
    "    print('PARAMS')\n",
    "    for k in param_keys:\n",
    "        v = getattr(config, k)\n",
    "        print(f'{k:<24s} = {v}')\n",
    "    print()\n",
    "\n",
    "    # print vocab\n",
    "    vocab_keys = [\n",
    "        'vocab_size', 'pad_token_id', 'unk_token_id', 'cls_token_id', 'sep_token_id'\n",
    "    ]\n",
    "    print('VOCAB')\n",
    "    for k in vocab_keys:\n",
    "        v = getattr(tokenizer, k)\n",
    "        print(f'{k:24s} = {v}')\n",
    "\n",
    "\n",
    "    # start to write GGUF file\n",
    "    gguf_writer = GGUFWriter(output_path, \"JinaBert\")\n",
    "\n",
    "    # write metadata\n",
    "    gguf_writer.add_name(repo_id)\n",
    "    gguf_writer.add_description('gguf model for embeddings.cpp')\n",
    "    gguf_writer.add_file_type(qtype)\n",
    "\n",
    "    # write model params\n",
    "    gguf_writer.add_uint32('vocab_size', config.vocab_size)\n",
    "    gguf_writer.add_uint32('hidden_size', config.hidden_size)\n",
    "    gguf_writer.add_uint32('num_hidden_layers', config.num_hidden_layers)\n",
    "    gguf_writer.add_uint32('num_attention_heads', config.num_attention_heads)\n",
    "    gguf_writer.add_uint32('intermediate_size', config.intermediate_size)\n",
    "    gguf_writer.add_uint32('type_vocab_size', config.type_vocab_size)\n",
    "    gguf_writer.add_uint32('pad_token_id', config.pad_token_id)\n",
    "    gguf_writer.add_float32('layer_norm_eps', config.layer_norm_eps)\n",
    "\n",
    "    # write the tokenizer special token(we only need to know [PAD])\n",
    "    KEY_PAD_ID = 'tokenizer.ggml.padding_token_id'\n",
    "    gguf_writer.add_int32(KEY_PAD_ID, tokenizer.pad_token_id)\n",
    "\n",
    "    # write tensors\n",
    "    print('TENSORS')\n",
    "    hidden_size = config.hidden_size\n",
    "    for name, data in model.state_dict().items():\n",
    "        # get correct dtype\n",
    "        if 'emb_ln' in name or 'norm1' in name or 'norm2' in name or 'bias' in name:\n",
    "            dtype = torch.float32\n",
    "        else:\n",
    "            dtype = dtype0\n",
    "        # if \"mixer.Wqkv.weight\" in name:\n",
    "        #     data_np = data.numpy()\n",
    "        #     data_q = data_np[:hidden_size, :]  # [768, 768]\n",
    "        #     data_k = data_np[hidden_size:2*hidden_size, :]  # [768, 768]\n",
    "        #     data_v = data_np[2*hidden_size:, :]  # [768, 768]\n",
    "        #     name_q = name + \".q\"\n",
    "        #     name_k = name + \".k\"\n",
    "        #     name_v = name + \".v\"\n",
    "\n",
    "        #     gguf_writer.add_tensor(name_q, data_q)\n",
    "        #     gguf_writer.add_tensor(name_k, data_k)\n",
    "        #     gguf_writer.add_tensor(name_v, data_v)\n",
    "\n",
    "        #     print(f'{name_q:64s} = {str(list(data_q.shape)):16s} {data_q.dtype} → {dtype}')\n",
    "        #     print(f'{name_k:64s} = {str(list(data_k.shape)):16s} {data_k.dtype} → {dtype}')\n",
    "        #     print(f'{name_v:64s} = {str(list(data_v.shape)):16s} {data_k.dtype} → {dtype}')\n",
    "        # elif \"mixer.Wqkv.bias\" in name:\n",
    "        #     data_np = data.numpy()\n",
    "        #     data_q = data_np[:hidden_size]  # [768, 768]\n",
    "        #     data_k = data_np[hidden_size:2*hidden_size]  # [768, 768]\n",
    "        #     data_v = data_np[2*hidden_size:]  # [768, 768]\n",
    "        #     name_q = name + \".q\"\n",
    "        #     name_k = name + \".k\"\n",
    "        #     name_v = name + \".v\"\n",
    "\n",
    "        #     gguf_writer.add_tensor(name_q, data_q)\n",
    "        #     gguf_writer.add_tensor(name_k, data_k)\n",
    "        #     gguf_writer.add_tensor(name_v, data_v)\n",
    "\n",
    "        #     print(f'{name_q:64s} = {str(list(data_q.shape)):16s} {data_q.dtype} → {dtype}')\n",
    "        #     print(f'{name_k:64s} = {str(list(data_k.shape)):16s} {data_k.dtype} → {dtype}')\n",
    "        #     print(f'{name_v:64s} = {str(list(data_v.shape)):16s} {data_k.dtype} → {dtype}')\n",
    "        # else:\n",
    "        # print info\n",
    "        shape_str = str(list(data.shape))\n",
    "        print(f'{name:64s} = {shape_str:16s} {data.dtype} → {dtype}')\n",
    "\n",
    "        # do conversion\n",
    "        data = data.to(dtype)\n",
    "\n",
    "        # add to gguf output\n",
    "        gguf_writer.add_tensor(name, data.numpy())\n",
    "\n",
    "    # execute and close writer\n",
    "    gguf_writer.write_header_to_file()\n",
    "    gguf_writer.write_kv_data_to_file()\n",
    "    gguf_writer.write_tensors_to_file()\n",
    "    gguf_writer.close()\n",
    "\n",
    "    # print success\n",
    "    print()\n",
    "    print(f'GGML model written to {output_path}')\n",
    "\n",
    "repo_id = 'jinaai/jina-clip-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JinaCLIPModel(\n",
      "  (text_model): HFTextEncoder(\n",
      "    (transformer): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30528, 768, padding_idx=0)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "      )\n",
      "      (emb_drop): Dropout(p=0.1, inplace=False)\n",
      "      (emb_ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (encoder): BertEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-11): 12 x Block(\n",
      "            (mixer): MHA(\n",
      "              (Wqkv): LinearResidual(in_features=768, out_features=2304, bias=True)\n",
      "              (inner_attn): SelfAttention(\n",
      "                (drop): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (inner_cross_attn): CrossAttention(\n",
      "                (drop): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
      "            (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (mlp): GLUMLP(\n",
      "              (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "              (act): GELU(approximate='none')\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
      "            (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): MeanPooler()\n",
      "    (proj): Identity()\n",
      "  )\n",
      "  (vision_model): EVAVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (rope): VisionRotaryEmbeddingFast()\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (inner_attn_ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (rope): VisionRotaryEmbeddingFast()\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): SwiGLU(\n",
      "          (w1): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (w2): Linear(in_features=768, out_features=2048, bias=True)\n",
      "          (act): SiLU()\n",
      "          (ffn_ln): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (w3): Linear(in_features=2048, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "    (patch_dropout): PatchDropout()\n",
      "  )\n",
      "  (visual_projection): Identity()\n",
      "  (text_projection): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModel.from_pretrained(repo_id, trust_remote_code=True)\n",
    "print(model)\n",
    "text_model = model.text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block(\n",
      "  (mixer): MHA(\n",
      "    (Wqkv): LinearResidual(in_features=768, out_features=2304, bias=True)\n",
      "    (inner_attn): SelfAttention(\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (inner_cross_attn): CrossAttention(\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
      "  (norm1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (mlp): GLUMLP(\n",
      "    (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "    (act): GELU(approximate='none')\n",
      "    (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
      "  (norm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "text_model = model.text_model\n",
    "print(text_model.transformer.encoder.layers[0])\n",
    "# print(text_model.transformer.embeddings.token_type_embeddings.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "use_flash_attn: False\n",
      "use_qk_norm: False\n",
      "fused_bias_fc: False\n",
      "window_size: [-1, -1]\n",
      "num_heads: 12, cross_attn: False, use_flash_attn: False, return_residual: True, window_size: [-1, -1]\n",
      "seqlen: 16, linear_biases: torch.Size([1, 12, 16, 16])\n",
      "JinaBertConfig {\n",
      "  \"_name_or_path\": \"jinaai/jina-bert-flash-implementation\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"jinaai/jina-bert-flash-implementation--configuration_bert.JinaBertConfig\",\n",
      "    \"AutoModel\": \"jinaai/jina-bert-flash-implementation--modeling_bert.BertModel\",\n",
      "    \"AutoModelForMaskedLM\": \"jinaai/jina-bert-flash-implementation--modeling_bert.BertForPreTraining\",\n",
      "    \"AutoModelForPreTraining\": \"jinaai/jina-bert-flash-implementation--modeling_bert.BertForPreTraining\"\n",
      "  },\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dense_seq_output\": false,\n",
      "  \"emb_pooler\": null,\n",
      "  \"fused_bias_fc\": false,\n",
      "  \"fused_dropout_add_ln\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"last_layer_subset\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mlp_checkpoint_lvl\": 0,\n",
      "  \"mlp_type\": \"glu\",\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_loras\": 5,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pad_vocab_size_multiple\": 1,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.48.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_flash_attn\": false,\n",
      "  \"use_qk_norm\": false,\n",
      "  \"vocab_size\": 30528,\n",
      "  \"window_size\": [\n",
      "    -1,\n",
      "    -1\n",
      "  ]\n",
      "}\n",
      "\n",
      "PARAMS\n",
      "vocab_size               = 30528\n",
      "hidden_size              = 768\n",
      "num_hidden_layers        = 12\n",
      "num_attention_heads      = 12\n",
      "intermediate_size        = 3072\n",
      "type_vocab_size          = 2\n",
      "pad_token_id             = 0\n",
      "\n",
      "VOCAB\n",
      "vocab_size               = 30528\n",
      "pad_token_id             = 0\n",
      "unk_token_id             = 100\n",
      "cls_token_id             = 101\n",
      "sep_token_id             = 102\n",
      "TENSORS\n",
      "transformer.embeddings.word_embeddings.weight                    = [30528, 768]     torch.float32 → torch.float32\n",
      "transformer.embeddings.token_type_embeddings.weight              = [2, 768]         torch.float32 → torch.float32\n",
      "transformer.emb_ln.weight                                        = [768]            torch.float32 → torch.float32\n",
      "transformer.emb_ln.bias                                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.mixer.Wqkv.weight                   = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.mixer.Wqkv.bias                     = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.mixer.out_proj.weight               = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.mixer.out_proj.bias                 = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.norm1.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.norm1.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.mlp.gated_layers.weight             = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.mlp.wo.weight                       = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.mlp.wo.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.norm2.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.0.norm2.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.mixer.Wqkv.weight                   = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.mixer.Wqkv.bias                     = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.mixer.out_proj.weight               = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.mixer.out_proj.bias                 = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.norm1.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.norm1.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.mlp.gated_layers.weight             = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.mlp.wo.weight                       = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.mlp.wo.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.norm2.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.1.norm2.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.mixer.Wqkv.weight                   = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.mixer.Wqkv.bias                     = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.mixer.out_proj.weight               = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.mixer.out_proj.bias                 = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.norm1.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.norm1.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.mlp.gated_layers.weight             = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.mlp.wo.weight                       = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.mlp.wo.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.norm2.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.2.norm2.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.mixer.Wqkv.weight                   = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.mixer.Wqkv.bias                     = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.mixer.out_proj.weight               = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.mixer.out_proj.bias                 = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.norm1.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.norm1.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.mlp.gated_layers.weight             = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.mlp.wo.weight                       = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.mlp.wo.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.norm2.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.3.norm2.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.mixer.Wqkv.weight                   = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.mixer.Wqkv.bias                     = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.mixer.out_proj.weight               = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.mixer.out_proj.bias                 = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.norm1.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.norm1.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.mlp.gated_layers.weight             = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.mlp.wo.weight                       = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.mlp.wo.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.norm2.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.4.norm2.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.mixer.Wqkv.weight                   = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.mixer.Wqkv.bias                     = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.mixer.out_proj.weight               = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.mixer.out_proj.bias                 = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.norm1.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.norm1.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.mlp.gated_layers.weight             = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.mlp.wo.weight                       = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.mlp.wo.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.norm2.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.5.norm2.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.mixer.Wqkv.weight                   = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.mixer.Wqkv.bias                     = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.mixer.out_proj.weight               = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.mixer.out_proj.bias                 = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.norm1.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.norm1.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.mlp.gated_layers.weight             = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.mlp.wo.weight                       = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.mlp.wo.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.norm2.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.6.norm2.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.mixer.Wqkv.weight                   = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.mixer.Wqkv.bias                     = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.mixer.out_proj.weight               = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.mixer.out_proj.bias                 = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.norm1.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.norm1.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.mlp.gated_layers.weight             = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.mlp.wo.weight                       = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.mlp.wo.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.norm2.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.7.norm2.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.mixer.Wqkv.weight                   = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.mixer.Wqkv.bias                     = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.mixer.out_proj.weight               = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.mixer.out_proj.bias                 = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.norm1.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.norm1.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.mlp.gated_layers.weight             = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.mlp.wo.weight                       = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.mlp.wo.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.norm2.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.8.norm2.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.mixer.Wqkv.weight                   = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.mixer.Wqkv.bias                     = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.mixer.out_proj.weight               = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.mixer.out_proj.bias                 = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.norm1.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.norm1.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.mlp.gated_layers.weight             = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.mlp.wo.weight                       = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.mlp.wo.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.norm2.weight                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.9.norm2.bias                          = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.mixer.Wqkv.weight                  = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.mixer.Wqkv.bias                    = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.mixer.out_proj.weight              = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.mixer.out_proj.bias                = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.norm1.weight                       = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.norm1.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.mlp.gated_layers.weight            = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.mlp.wo.weight                      = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.mlp.wo.bias                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.norm2.weight                       = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.10.norm2.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.mixer.Wqkv.weight                  = [2304, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.mixer.Wqkv.bias                    = [2304]           torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.mixer.out_proj.weight              = [768, 768]       torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.mixer.out_proj.bias                = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.norm1.weight                       = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.norm1.bias                         = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.mlp.gated_layers.weight            = [6144, 768]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.mlp.wo.weight                      = [768, 3072]      torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.mlp.wo.bias                        = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.norm2.weight                       = [768]            torch.float32 → torch.float32\n",
      "transformer.encoder.layers.11.norm2.bias                         = [768]            torch.float32 → torch.float32\n",
      "\n",
      "GGML model written to ../models/jina-clip-v1.fp32.gguf\n"
     ]
    }
   ],
   "source": [
    "convert_hf(repo_id, \"../models/jina-clip-v1.fp32.gguf\", float_type=\"f32\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
